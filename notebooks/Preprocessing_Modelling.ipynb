{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 81,
      "id": "f7401dd9-4800-4a00-9c53-77f4c1022659",
      "metadata": {
        "id": "f7401dd9-4800-4a00-9c53-77f4c1022659"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import random, time\n",
        "import warnings, os\n",
        "\n",
        "from sklearn.feature_selection import VarianceThreshold\n",
        "from sklearn.preprocessing import StandardScaler, OneHotEncoder, LabelEncoder\n",
        "from sklearn.impute import SimpleImputer, KNNImputer\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV, RandomizedSearchCV, learning_curve\n",
        "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score\n",
        "from sklearn.metrics import classification_report, confusion_matrix, roc_curve\n",
        "\n",
        "from scipy.stats import uniform, randint\n",
        "\n",
        "from imblearn.under_sampling import RandomUnderSampler\n",
        "from imblearn.over_sampling import ADASYN, BorderlineSMOTE\n",
        "from imblearn.combine import SMOTEENN, SMOTETomek\n",
        "from imblearn.ensemble import BalancedBaggingClassifier, EasyEnsembleClassifier, BalancedRandomForestClassifier\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 82,
      "id": "69b30aec-3e5d-4dd7-a702-63055d010306",
      "metadata": {
        "id": "69b30aec-3e5d-4dd7-a702-63055d010306"
      },
      "outputs": [],
      "source": [
        "# Ignore Warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Suppress scientific notation for pandas display\n",
        "pd.options.display.float_format = '{:.4f}'.format"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 83,
      "id": "dbb8b439-3686-4a92-bec9-89e4883eaf39",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dbb8b439-3686-4a92-bec9-89e4883eaf39",
        "outputId": "e57b9ad6-b2a1-4836-97b8-48e4e59f03d8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processed sampled dataframes loaded.\n",
            "Shape of numeric_sampled: (13107, 970)\n",
            "Shape of date_sampled: (13107, 1157)\n",
            "Shape of categorical_sampled: (13107, 2141)\n"
          ]
        }
      ],
      "source": [
        "# Load the processed sampled dataframes\n",
        "# Ensure they were saved in the directory : processed_data/\n",
        "numeric_sampled = pd.read_csv('processed_data/num_sampled.csv')\n",
        "categorical_sampled = pd.read_csv('processed_data/cat_sampled.csv')\n",
        "date_sampled = pd.read_csv('processed_data/date_sampled.csv')\n",
        "\n",
        "\n",
        "print(\"Processed sampled dataframes loaded.\")\n",
        "print(f\"Shape of numeric_sampled: {numeric_sampled.shape}\")\n",
        "print(f\"Shape of date_sampled: {date_sampled.shape}\")\n",
        "print(f\"Shape of categorical_sampled: {categorical_sampled.shape}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 84,
      "id": "f66d00ac-ef8a-4092-9a45-b81baa3f3575",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f66d00ac-ef8a-4092-9a45-b81baa3f3575",
        "outputId": "bff90367-c423-44e9-aa6e-d6f89f9bc4a4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Step 0: Original shape of train_numeric_sampled: (13107, 970)\n",
            "Step 1: Dropped 0 columns with 100% missing values.\n",
            "Step 2: Dropped 516 low/zero variance columns.\n",
            "Step 3: Imputed missing values in 452 columns using median.\n",
            "Step 4: Scaled 452 numeric columns using StandardScaler.\n",
            "Step 5: Deferring dropping highly correlated columns (multicollinearity) until after train-test split.\n",
            "Step 6: Deferring dropping low-correlation columns until after train-test split.\n",
            "\n",
            "âœ… Final shape of numeric after column-only preprocessing: (13107, 454)\n",
            "ðŸ”» Total columns dropped so far: 516\n",
            "ðŸ“ Saved 0 columns dropped in '100_percent_missing' to drop_logs/dropped_num_100_percent_missing.txt\n",
            "ðŸ“ Saved 516 columns dropped in 'low_variance' to drop_logs/dropped_num_low_variance.txt\n",
            "ðŸ“ Saved 452 columns dropped in 'imputed' to drop_logs/dropped_num_imputed.txt\n"
          ]
        }
      ],
      "source": [
        "# Step 0: Copy the numeric dataset\n",
        "df_numeric = numeric_sampled.copy()\n",
        "original_shape = df_numeric.shape\n",
        "print(f\"Step 0: Original shape of train_numeric_sampled: {original_shape}\")\n",
        "\n",
        "# âœ… Maintain index integrity â€” no row drops\n",
        "# All preprocessing will be column-wise only\n",
        "\n",
        "# Initialize logs\n",
        "dropped_columns = {\n",
        "    '100_percent_missing': [],\n",
        "    'low_variance': [],\n",
        "    'low_correlation': [], # Deferring this step until after train-test split\n",
        "    'high_multicollinearity': [], # Deferring this step until after train-test split\n",
        "    'imputed': []\n",
        "}\n",
        "\n",
        "# Step 1: Drop columns with 100% missing values\n",
        "missing_100 = df_numeric.columns[df_numeric.isnull().mean() == 1.0].tolist()\n",
        "df_numeric.drop(columns=missing_100, inplace=True)\n",
        "dropped_columns['100_percent_missing'] = missing_100\n",
        "print(f\"Step 1: Dropped {len(missing_100)} columns with 100% missing values.\")\n",
        "\n",
        "# Step 2: Drop low/zero variance columns\n",
        "id_cols = ['Id', 'Response']\n",
        "feature_data = df_numeric.drop(columns=id_cols, errors='ignore')\n",
        "\n",
        "var_thresh = VarianceThreshold(threshold=0.01)\n",
        "var_thresh.fit(feature_data)\n",
        "selected_features = feature_data.columns[var_thresh.get_support()].tolist()\n",
        "dropped_variance = list(set(feature_data.columns) - set(selected_features))\n",
        "df_numeric = pd.concat([df_numeric[id_cols], feature_data[selected_features]], axis=1)\n",
        "dropped_columns['low_variance'] = dropped_variance\n",
        "print(f\"Step 2: Dropped {len(dropped_variance)} low/zero variance columns.\")\n",
        "\n",
        "# Step 3: Imputation using median (no row drops)\n",
        "# Simplified for now, consider model-based imputation when compute available\n",
        "missing_cols = [col for col in df_numeric.columns if df_numeric[col].isnull().any()]\n",
        "for col in missing_cols:\n",
        "    median_val = df_numeric[col].median() # Using overall median for now\n",
        "    df_numeric[col].fillna(median_val, inplace=True)\n",
        "dropped_columns['imputed'] = missing_cols\n",
        "print(f\"Step 3: Imputed missing values in {len(missing_cols)} columns using median.\")\n",
        "# Data leakage possible but insignificant.\n",
        "\n",
        "# Step 4: Scaling (standardization)\n",
        "# Perform scaling before multicollinearity check\n",
        "scaler = StandardScaler()\n",
        "scaled_features = scaler.fit_transform(df_numeric.drop(columns=id_cols, errors='ignore'))\n",
        "df_scaled = pd.DataFrame(scaled_features, columns=df_numeric.drop(columns=id_cols, errors='ignore').columns, index=df_numeric.index)\n",
        "df_numeric_scaled = pd.concat([df_numeric[id_cols], df_scaled], axis=1)\n",
        "print(f\"Step 4: Scaled {len(df_scaled.columns)} numeric columns using StandardScaler.\")\n",
        "\n",
        "# Step 5: Defer dropping highly correlated columns (multicollinearity) until after train-test split\n",
        "# This step can be done after splitting to avoid data leakage from the test set's correlation structure\n",
        "print(\"Step 5: Deferring dropping highly correlated columns (multicollinearity) until after train-test split.\")\n",
        "# dropped_columns['high_multicollinearity'] = high_corr_pairs # Commenting out the dropping part\n",
        "\n",
        "# Step 6: Defer dropping low correlation to Response until after train-test split\n",
        "# This step should be done after splitting to avoid data leakage\n",
        "print(\"Step 6: Deferring dropping low-correlation columns until after train-test split.\")\n",
        "# dropped_columns['low_correlation'] = dropped_corr # Commenting out the dropping part\n",
        "\n",
        "\n",
        "# Final output\n",
        "numeric = df_numeric_scaled.copy()\n",
        "final_shape = numeric.shape\n",
        "print(f\"\\nâœ… Final shape of numeric after column-only preprocessing: {final_shape}\")\n",
        "# Recalculating total columns dropped based on steps performed\n",
        "total_dropped = len(dropped_columns['100_percent_missing']) + len(dropped_columns['low_variance'])\n",
        "print(f\"ðŸ”» Total columns dropped so far: {total_dropped}\")\n",
        "\n",
        "# Save dropped column names to files\n",
        "os.makedirs(\"drop_logs\", exist_ok=True)\n",
        "for step, cols in dropped_columns.items():\n",
        "    if step not in ['low_correlation', 'high_multicollinearity']: # Only save logs for steps performed\n",
        "        file_path = f\"drop_logs/dropped_num_{step}.txt\"\n",
        "        with open(file_path, \"w\") as f:\n",
        "            for col in cols:\n",
        "                f.write(col + \"\\n\")\n",
        "        print(f\"ðŸ“ Saved {len(cols)} columns dropped in '{step}' to {file_path}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 85,
      "id": "6f3eb204-3c91-4d48-a09b-67e07779ce3c",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6f3eb204-3c91-4d48-a09b-67e07779ce3c",
        "outputId": "0e9cd1aa-e84d-4b4c-d0fd-e0fad0947a67"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Step 0: Original shape of train_categorical_sampled: (13107, 2141)\n",
            "Step 1: Dropped 1262 columns with 100% missing values.\n",
            "Step 2: Dropped 780 constant categorical features.\n",
            "Step 3: Dropped 0 high-cardinality features (>50 unique values), excluding 'Id'.\n",
            "Step 4: Imputed missing values in 98 columns with 'Missing' label.\n",
            "Step 5: Deferring Mean Response Encoding until after train-test split.\n",
            "Step 6: Deferring dropping low-correlation features until after train-test split.\n",
            "\n",
            "âœ… Final shape of categorical after preprocessing: (13107, 99)\n",
            "ðŸ”» Total columns dropped so far: 2042\n",
            "ðŸ“ Saved 0 columns dropped in '100_percent_missing' to drop_logs/dropped_cat_100_percent_missing.txt\n",
            "ðŸ“ Saved 780 columns dropped in 'constant' to drop_logs/dropped_cat_constant.txt\n",
            "ðŸ“ Saved 0 columns dropped in 'high_cardinality' to drop_logs/dropped_cat_high_cardinality.txt\n",
            "ðŸ“ Saved 98 columns dropped in 'imputed' to drop_logs/dropped_cat_imputed.txt\n",
            "ðŸ“ Saved 1262 columns dropped in '100_percent_percent_missing' to drop_logs/dropped_cat_100_percent_percent_missing.txt\n"
          ]
        }
      ],
      "source": [
        "# Step 0: Start with a copy of categorcial dataset\n",
        "df_categorical = categorical_sampled.copy()\n",
        "original_shape = df_categorical.shape\n",
        "print(f\"Step 0: Original shape of train_categorical_sampled: {original_shape}\")\n",
        "\n",
        "# Initialize logs\n",
        "dropped_columns = {\n",
        "    '100_percent_missing': [],\n",
        "    'constant': [],\n",
        "    'high_cardinality': [],\n",
        "    'low_correlation': [], # Deferring this step until after train-test split\n",
        "    'imputed': []\n",
        "}\n",
        "\n",
        "# Step 1: Drop 100% missing value features\n",
        "missing_100 = df_categorical.columns[df_categorical.isnull().mean() == 1.0].tolist()\n",
        "df_categorical.drop(columns=missing_100, inplace=True)\n",
        "dropped_columns['100_percent_percent_missing'] = missing_100\n",
        "print(f\"Step 1: Dropped {len(missing_100)} columns with 100% missing values.\")\n",
        "\n",
        "# Step 2: Drop constant features (only one unique value)\n",
        "constant_cols = [col for col in df_categorical.columns if df_categorical[col].nunique() == 1]\n",
        "df_categorical.drop(columns=constant_cols, inplace=True)\n",
        "dropped_columns['constant'] = constant_cols\n",
        "print(f\"Step 2: Dropped {len(constant_cols)} constant categorical features.\")\n",
        "\n",
        "# Step 3: Cardinality analysis\n",
        "# Exclude 'Id' from high cardinality check\n",
        "high_card_cols = [col for col in df_categorical.columns if col != 'Id' and df_categorical[col].nunique() > 50]\n",
        "df_categorical.drop(columns=high_card_cols, inplace=True)\n",
        "dropped_columns['high_cardinality'] = high_card_cols\n",
        "print(f\"Step 3: Dropped {len(high_card_cols)} high-cardinality features (>50 unique values), excluding 'Id'.\")\n",
        "\n",
        "# Step 4: Missing value imputation - Using a placeholder 'Missing' for now, consider more sophisticated imputation after merging\n",
        "missing_cols = [col for col in df_categorical.columns if df_categorical[col].isnull().any()]\n",
        "for col in missing_cols:\n",
        "    df_categorical[col].fillna(\"Missing\", inplace=True)\n",
        "dropped_columns['imputed'] = missing_cols\n",
        "print(f\"Step 4: Imputed missing values in {len(missing_cols)} columns with 'Missing' label.\")\n",
        "# TODO: Consider more sophisticated imputation methods (e.g., mode imputation or model-based) after merging dataframes and train-test split.\n",
        "\n",
        "\n",
        "# Step 5: Defer Mean Response Encoding until after train-test split\n",
        "# Mean Response Encoding can cause data leakage if done before splitting\n",
        "print(\"Step 5: Deferring Mean Response Encoding until after train-test split.\")\n",
        "# Encoding will be done after the split\n",
        "\n",
        "\n",
        "# Step 6: Defer dropping low correlation features until after train-test split\n",
        "# This step should be done after splitting to avoid data leakage\n",
        "print(\"Step 6: Deferring dropping low-correlation features until after train-test split.\")\n",
        "# dropped_columns['low_correlation'] = low_corr_features # Commenting out the dropping part\n",
        "\n",
        "# Final output\n",
        "categorical = df_categorical.copy()\n",
        "final_shape = categorical.shape\n",
        "print(f\"\\nâœ… Final shape of categorical after preprocessing: {final_shape}\")\n",
        "# Recalculating total columns dropped based on steps performed\n",
        "total_dropped = len(dropped_columns['100_percent_percent_missing']) + len(dropped_columns['constant']) + len(dropped_columns['high_cardinality'])\n",
        "print(f\"ðŸ”» Total columns dropped so far: {total_dropped}\")\n",
        "\n",
        "\n",
        "# Save dropped column names to files\n",
        "os.makedirs(\"drop_logs\", exist_ok=True)\n",
        "for step, cols in dropped_columns.items():\n",
        "    if step not in ['low_correlation']: # Only save logs for steps performed\n",
        "        file_path = f\"drop_logs/dropped_cat_{step}.txt\"\n",
        "        with open(file_path, \"w\") as f:\n",
        "            for col in cols:\n",
        "                f.write(col + \"\\n\")\n",
        "        print(f\"ðŸ“ Saved {len(cols)} columns dropped in '{step}' to {file_path}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 86,
      "id": "256e10ff",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "256e10ff",
        "outputId": "4a398209-322b-4107-cb81-e206bd7e3d18"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Step 0: Original shape of date_sampled: (13107, 1157)\n",
            "Step 1: Dropped 10 columns with 100% missing values.\n",
            "Step 2: Dropped 1 constant date features.\n",
            "Step 4: Imputed missing values in 1145 derived date columns using median.\n",
            "\n",
            "âœ… Final shape of date after preprocessing: (13107, 1146)\n",
            "ðŸ”» Total columns dropped so far: 11\n",
            "ðŸ“ Saved 10 columns dropped in '100_percent_missing' to drop_logs/dropped_date_100_percent_missing.txt\n",
            "ðŸ“ Saved 1 columns dropped in 'constant' to drop_logs/dropped_date_constant.txt\n",
            "ðŸ“ Saved 0 columns dropped in 'derived_features_missing' to drop_logs/dropped_date_derived_features_missing.txt\n",
            "ðŸ“ Saved 1145 columns dropped in 'imputed' to drop_logs/dropped_date_imputed.txt\n"
          ]
        }
      ],
      "source": [
        "# Step 0: Start with a copy of date dataset\n",
        "df_date = date_sampled.copy()\n",
        "original_shape = df_date.shape\n",
        "print(f\"Step 0: Original shape of date_sampled: {original_shape}\")\n",
        "\n",
        "# Initialize logs\n",
        "dropped_columns = {\n",
        "    '100_percent_missing': [],\n",
        "    'constant': [],\n",
        "    'derived_features_missing': [] # Log for rows dropped due to missingness for derived features\n",
        "}\n",
        "\n",
        "# Step 1: Drop 100% missing value features\n",
        "missing_100 = df_date.columns[df_date.isnull().mean() == 1.0].tolist()\n",
        "df_date.drop(columns=missing_100, inplace=True)\n",
        "dropped_columns['100_percent_missing'] = missing_100\n",
        "print(f\"Step 1: Dropped {len(missing_100)} columns with 100% missing values.\")\n",
        "\n",
        "# Step 2: Drop constant features (only one unique value)\n",
        "constant_cols = [col for col in df_date.columns if df_date[col].nunique() == 1]\n",
        "df_date.drop(columns=constant_cols, inplace=True)\n",
        "dropped_columns['constant'] = constant_cols\n",
        "print(f\"Step 2: Dropped {len(constant_cols)} constant date features.\")\n",
        "\n",
        "# Step 3: Convert to datetime and create derived features\n",
        "# Convert date columns to datetime objects, coercing errors\n",
        "date_cols = [col for col in df_date.columns if col != 'Id']\n",
        "for col in date_cols:\n",
        "    df_date[col] = pd.to_datetime(df_date[col], errors='coerce')\n",
        "\n",
        "# Create derived features (e.g., day of week, day of year, hour, minute, etc.)\n",
        "# This can be done after merging to handle potential missingness across different date columns consistently.\n",
        "# For now, we will just convert to numeric representation (e.g., Unix timestamp or days since a reference date)\n",
        "# and impute missing values.\n",
        "\n",
        "# Example: Convert to days since the minimum date in the dataset\n",
        "min_date = df_date[date_cols].min().min()\n",
        "for col in date_cols:\n",
        "    df_date[col + '_days_since_min'] = (df_date[col] - min_date).dt.days\n",
        "\n",
        "# Drop original datetime columns after creating derived features\n",
        "df_date.drop(columns=date_cols, inplace=True)\n",
        "\n",
        "# Step 4: Imputation for derived date features - Using median for now, consider model-based imputation after merging\n",
        "derived_date_cols = [col for col in df_date.columns if col.endswith('_days_since_min')]\n",
        "missing_cols_derived = [col for col in derived_date_cols if df_date[col].isnull().any()]\n",
        "for col in missing_cols_derived:\n",
        "    median_val = df_date[col].median() # Using overall median for now\n",
        "    df_date[col].fillna(median_val, inplace=True)\n",
        "dropped_columns['imputed'] = missing_cols_derived\n",
        "print(f\"Step 4: Imputed missing values in {len(missing_cols_derived)} derived date columns using median.\")\n",
        "# TODO: Consider more sophisticated imputation methods (e.g., model-based) after merging dataframes and train-test split.\n",
        "\n",
        "\n",
        "# Final output\n",
        "date = df_date.copy()\n",
        "final_shape = date.shape\n",
        "print(f\"\\nâœ… Final shape of date after preprocessing: {final_shape}\")\n",
        "# Recalculating total columns dropped based on steps performed\n",
        "total_dropped = len(dropped_columns['100_percent_missing']) + len(dropped_columns['constant'])\n",
        "print(f\"ðŸ”» Total columns dropped so far: {total_dropped}\")\n",
        "\n",
        "# Save dropped column names to files\n",
        "os.makedirs(\"drop_logs\", exist_ok=True)\n",
        "for step, cols in dropped_columns.items():\n",
        "    file_path = f\"drop_logs/dropped_date_{step}.txt\"\n",
        "    with open(file_path, \"w\") as f:\n",
        "        for col in cols:\n",
        "            f.write(col + \"\\n\")\n",
        "    print(f\"ðŸ“ Saved {len(cols)} columns dropped in '{step}' to {file_path}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 87,
      "id": "2978ea54",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2978ea54",
        "outputId": "23562ca4-6303-411b-c910-0e9a06ac9354"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Step 1: Merging preprocessed dataframes...\n",
            "âœ… Dataframes merged successfully.\n",
            "Shape of the merged dataframe: (13107, 4266)\n"
          ]
        }
      ],
      "source": [
        "# Step 1: Merge the preprocessed dataframes\n",
        "print(\"Step 1: Merging preprocessed dataframes...\")\n",
        "# Taking 'Id' as the common column for merging\n",
        "merged_data = numeric_sampled.merge(categorical_sampled, on='Id', how='left')\n",
        "merged_data = merged_data.merge(date_sampled, on='Id', how='left')\n",
        "\n",
        "print(\"âœ… Dataframes merged successfully.\")\n",
        "print(f\"Shape of the merged dataframe: {merged_data.shape}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 88,
      "id": "b28dad55",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b28dad55",
        "outputId": "d2a081fa-f720-4730-936d-d452b8087448"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Step 2: Splitting data into training and testing sets...\n",
            "âœ… Data split successfully.\n",
            "Shape of X_train: (10485, 4264)\n",
            "Shape of X_test: (2622, 4264)\n",
            "Shape of y_train: (10485,)\n",
            "Shape of y_test: (2622,)\n"
          ]
        }
      ],
      "source": [
        "# Step 2: Split the merged data into training and testing sets\n",
        "print(\"\\nStep 2: Splitting data into training and testing sets...\")\n",
        "\n",
        "# Separate features (X) and target (y)\n",
        "# 'Response' is the target variable and 'Id' is an identifier\n",
        "X = merged_data.drop(columns=['Response', 'Id'])\n",
        "y = merged_data['Response']\n",
        "ids = merged_data['Id'] # Keep track of IDs for potential later use\n",
        "\n",
        "# Split data into training and testing sets (e.g., 80% train, 20% test)\n",
        "X_train, X_test, y_train, y_test, ids_train, ids_test = train_test_split(\n",
        "    X, y, ids, test_size=0.2, random_state=42, stratify=y\n",
        ")\n",
        "\n",
        "print(\"âœ… Data split successfully.\")\n",
        "print(f\"Shape of X_train: {X_train.shape}\")\n",
        "print(f\"Shape of X_test: {X_test.shape}\")\n",
        "print(f\"Shape of y_train: {y_train.shape}\")\n",
        "print(f\"Shape of y_test: {y_test.shape}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6f80fb72",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6f80fb72",
        "outputId": "a03cf02a-fd88-4c7f-dad3-374ae68d10fa"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Step 3: Applying deferred preprocessing steps...\n",
            "- Identified 3386 numeric columns and 878 non-numeric columns.\n",
            "- Handling multicollinearity for numeric features...\n"
          ]
        }
      ],
      "source": [
        "# Step 3: Implement Deferred Preprocessing Steps on Training Data\n",
        "\n",
        "print(\"\\nStep 3: Applying deferred preprocessing steps...\")\n",
        "\n",
        "# Separate numeric and non-numeric columns for targeted preprocessing\n",
        "numeric_cols_after_split = X_train.select_dtypes(include=np.number).columns.tolist()\n",
        "non_numeric_cols_after_split = X_train.select_dtypes(exclude=np.number).columns.tolist()\n",
        "\n",
        "print(f\"- Identified {len(numeric_cols_after_split)} numeric columns and {len(non_numeric_cols_after_split)} non-numeric columns.\")\n",
        "\n",
        "\n",
        "# --- Deferred Step: Handle Multicollinearity (Numeric Features) ---\n",
        "print(\"- Handling multicollinearity for numeric features...\")\n",
        "if numeric_cols_after_split:\n",
        "    corr_matrix = X_train[numeric_cols_after_split].corr().abs()\n",
        "\n",
        "    # Select upper triangle of correlation matrix\n",
        "    upper = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1).astype(bool))\n",
        "\n",
        "    # Find features with correlation greater than 0.9\n",
        "    to_drop_high_corr = [column for column in upper.columns if any(upper[column] > 0.9)]\n",
        "\n",
        "    print(f\"  Identified {len(to_drop_high_corr)} highly correlated numeric columns to drop.\")\n",
        "\n",
        "    # Drop highly correlated features from both training and testing sets\n",
        "    X_train = X_train.drop(columns=to_drop_high_corr)\n",
        "    X_test = X_test.drop(columns=to_drop_high_corr)\n",
        "    print(f\"  Dropped highly correlated columns from X_train and X_test.\")\n",
        "else:\n",
        "    print(\"  No numeric columns to check for multicollinearity.\")\n",
        "\n",
        "\n",
        "# --- Deferred Step: Mean Response Encoding (Categorical Features) ---\n",
        "# Apply Mean Response Encoding to non-numeric features in X_train and X_test\n",
        "print(\"\\n- Applying Mean Response Encoding to non-numeric features...\")\n",
        "if non_numeric_cols_after_split:\n",
        "    # Ensure y_train aligns with X_train\n",
        "    temp_train = X_train[non_numeric_cols_after_split].copy()\n",
        "    temp_train['Response'] = y_train\n",
        "\n",
        "    mean_response_features_train = pd.DataFrame()\n",
        "    mean_response_features_test = pd.DataFrame()\n",
        "\n",
        "    for col in non_numeric_cols_after_split:\n",
        "        if col in temp_train.columns:\n",
        "            # Calculate means from training data only\n",
        "            means = temp_train.groupby(col)['Response'].mean()\n",
        "            overall_mean = temp_train['Response'].mean()\n",
        "\n",
        "            # Apply to both train and test sets\n",
        "            X_train[col + '_mean_response'] = X_train[col].map(means).fillna(overall_mean)\n",
        "            X_test[col + '_mean_response'] = X_test[col].map(means).fillna(overall_mean)\n",
        "\n",
        "    # Drop original non-numeric columns\n",
        "    X_train = X_train.drop(columns=non_numeric_cols_after_split)\n",
        "    X_test = X_test.drop(columns=non_numeric_cols_after_split)\n",
        "    print(f\"  Applied Mean Response Encoding and dropped original non-numeric columns.\")\n",
        "else:\n",
        "    print(\"  No non-numeric columns to encode.\")\n",
        "\n",
        "\n",
        "# --- Deferred Step: Drop Low Correlation Features (Now all features should be numeric) ---\n",
        "# Calculate correlations with the target variable 'y_train'\n",
        "print(\"\\n- Dropping low correlation features...\")\n",
        "# Ensure y_train aligns with X_train after previous drops and encoding\n",
        "# Re-create a temporary dataframe with current X_train and y_train\n",
        "train_data_with_target_numeric = X_train.copy()\n",
        "train_data_with_target_numeric['Response'] = y_train\n",
        "\n",
        "\n",
        "correlations_with_target = train_data_with_target_numeric.corr()['Response'].abs().sort_values(ascending=False)\n",
        "\n",
        "# Define a correlation threshold (e.g., 0.01, adjust as needed)\n",
        "corr_threshold = 0.01\n",
        "\n",
        "# Find features with correlation less than the threshold (excluding 'Response' itself)\n",
        "to_drop_low_corr = correlations_with_target[correlations_with_target < corr_threshold].index.tolist()\n",
        "if 'Response' in to_drop_low_corr:\n",
        "    to_drop_low_corr.remove('Response') # Ensure target is not dropped\n",
        "\n",
        "print(f\"  Identified {len(to_drop_low_corr)} low correlation features to drop.\")\n",
        "\n",
        "# Drop low correlation features from both training and testing sets\n",
        "X_train = X_train.drop(columns=to_drop_low_corr)\n",
        "X_test = X_test.drop(columns=to_drop_low_corr)\n",
        "print(f\"  Dropped low correlation features from X_train and X_test.\")\n",
        "\n",
        "\n",
        "# --- Deferred Step: More Sophisticated Imputation (Placeholder) ---\n",
        "print(\"\\n- Deferring more sophisticated imputation methods for now.\")\n",
        "# Implement more sophisticated imputation methods (e.g., KNNImputer, IterativeImputer) here if needed.\n",
        "# This would involve fitting the imputer on X_train and transforming both X_train and X_test.\n",
        "\n",
        "\n",
        "print(\"\\nâœ… Deferred preprocessing steps applied to training and testing data.\")\n",
        "print(f\"Final shape of X_train after deferred steps: {X_train.shape}\")\n",
        "print(f\"Final shape of X_test after deferred steps: {X_test.shape}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "386122c0",
      "metadata": {
        "id": "386122c0"
      },
      "source": [
        "## Baseline model.\n",
        "Train a simple baseline model (e.g., Logistic Regression) to establish a performance benchmark.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6c79a9e5"
      },
      "source": [
        "# Impute missing values after train-test split and deferred preprocessing\n",
        "print(\"\\nImputing remaining missing values after deferred preprocessing...\")\n",
        "\n",
        "# Initialize imputer (using median for now, can be changed to KNNImputer etc.)\n",
        "imputer = SimpleImputer(strategy='median')\n",
        "\n",
        "# Fit on training data and transform both training and testing data\n",
        "X_train_imputed = imputer.fit_transform(X_train)\n",
        "X_test_imputed = imputer.transform(X_test)\n",
        "\n",
        "print(\"âœ… Imputation of remaining missing values complete.\")\n",
        "print(f\"Shape of X_train_imputed: {X_train_imputed.shape}\")\n",
        "print(f\"Shape of X_test_imputed: {X_test_imputed.shape}\")"
      ],
      "id": "6c79a9e5",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "MyBy5e_R1OKI",
      "metadata": {
        "id": "MyBy5e_R1OKI"
      },
      "outputs": [],
      "source": [
        "# Instantiate the model\n",
        "baseline_model = LogisticRegression()\n",
        "\n",
        "# Train the model using the imputed data\n",
        "baseline_model.fit(X_train_imputed, y_train)\n",
        "\n",
        "# Evaluate the model\n",
        "y_pred = baseline_model.predict(X_test_imputed)\n",
        "\n",
        "# Calculate various evaluation metrics\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "precision = precision_score(y_test, y_pred)\n",
        "recall = recall_score(y_test, y_pred)\n",
        "f1 = f1_score(y_test, y_pred)\n",
        "auc = roc_auc_score(y_test, baseline_model.predict_proba(X_test_imputed)[:, 1]) # Calculate AUC\n",
        "\n",
        "# Print the evaluation metrics\n",
        "print(f\"Baseline Logistic Regression Model Accuracy: {accuracy:.4f}\")\n",
        "print(f\"Baseline Logistic Regression Model Precision: {precision:.4f}\")\n",
        "print(f\"Baseline Logistic Regression Model Recall: {recall:.4f}\")\n",
        "print(f\"Baseline Logistic Regression Model F1-score: {f1:.4f}\")\n",
        "print(f\"Baseline Logistic Regression Model AUC: {auc:.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e8ea6988",
      "metadata": {
        "id": "e8ea6988"
      },
      "source": [
        "## Multiple model training\n",
        "Train several different machine learning models suitable for the classification task.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1c7cf8ce",
      "metadata": {
        "id": "1c7cf8ce"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "# 1. Define the dictionary of models\n",
        "models = {\n",
        "    'LogisticRegression': LogisticRegression(max_iter=1000), # Increased max_iter for convergence\n",
        "    'RandomForest': RandomForestClassifier(random_state=42),\n",
        "    'GradientBoosting': GradientBoostingClassifier(random_state=42),\n",
        "    'SVC': SVC(probability=True, random_state=42), # Added probability=True for potential later use with metrics like AUC\n",
        "    'KNeighbors': KNeighborsClassifier()\n",
        "}\n",
        "\n",
        "trained_models = {}\n",
        "\n",
        "# 2. Iterate through the models and train them\n",
        "print(\"Step 1: Training machine learning models...\")\n",
        "for name, model in models.items():\n",
        "    print(f\"  Training {name}...\")\n",
        "    model.fit(X_train_imputed, y_train)\n",
        "    trained_models[name] = model\n",
        "    print(f\"  {name} training complete.\")\n",
        "\n",
        "# 3. Store the trained models (already done in the loop)\n",
        "print(\"\\nâœ… All models trained successfully.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "791b0724",
      "metadata": {
        "id": "791b0724"
      },
      "source": [
        "## Model evaluation\n",
        "Evaluate all trained models using appropriate classification metrics (e.g., Accuracy, Precision, Recall, F1-score, AUC).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2a9d7f48",
      "metadata": {
        "id": "2a9d7f48"
      },
      "outputs": [],
      "source": [
        "evaluation_metrics = {}\n",
        "print(\"Initialized evaluation_metrics dictionary.\")\n",
        "print(\"\\nEvaluating trained models...\")\n",
        "for name, model in trained_models.items():\n",
        "    print(f\"  Evaluating {name}...\")\n",
        "    y_pred = model.predict(X_test_imputed)\n",
        "\n",
        "    accuracy = accuracy_score(y_test, y_pred)\n",
        "    precision = precision_score(y_test, y_pred)\n",
        "    recall = recall_score(y_test, y_pred)\n",
        "    f1 = f1_score(y_test, y_pred)\n",
        "\n",
        "    model_metrics = {\n",
        "        'Accuracy': accuracy,\n",
        "        'Precision': precision,\n",
        "        'Recall': recall,\n",
        "        'F1-score': f1\n",
        "    }\n",
        "\n",
        "    # Calculate AUC if the model supports probability predictions\n",
        "    if hasattr(model, 'predict_proba'):\n",
        "        y_pred_proba = model.predict_proba(X_test_imputed)[:, 1] # Probability of the positive class\n",
        "        auc = roc_auc_score(y_test, y_pred_proba)\n",
        "        model_metrics['AUC'] = auc\n",
        "\n",
        "    evaluation_metrics[name] = model_metrics\n",
        "\n",
        "    print(f\"    Accuracy: {accuracy:.4f}\")\n",
        "    print(f\"    Precision: {precision:.4f}\")\n",
        "    print(f\"    Recall: {recall:.4f}\")\n",
        "    print(f\"    F1-score: {f1:.4f}\")\n",
        "    if 'AUC' in model_metrics:\n",
        "        print(f\"    AUC: {auc:.4f}\")\n",
        "\n",
        "print(\"\\nâœ… Model evaluation complete.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "388905a7",
      "metadata": {
        "id": "388905a7"
      },
      "source": [
        "## Visual metrics\n",
        "Visualize the evaluation metrics to compare the performance of different models.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0b662bbe",
      "metadata": {
        "id": "0b662bbe"
      },
      "outputs": [],
      "source": [
        "# 1. Convert the evaluation_metrics dictionary into a pandas DataFrame\n",
        "eval_df = pd.DataFrame(evaluation_metrics).T # Transpose to have models as rows\n",
        "\n",
        "# 2. Create bar plots for each metric\n",
        "metrics_to_plot = ['Accuracy', 'AUC', 'F1-score', 'Precision', 'Recall']\n",
        "\n",
        "print(\"Generating plots for evaluation metrics...\")\n",
        "\n",
        "# Calculate the number of rows needed (ceil(num_metrics / 2))\n",
        "n_metrics = len(metrics_to_plot)\n",
        "nrows = (n_metrics + 1) // 2\n",
        "ncols = 2\n",
        "\n",
        "fig, axes = plt.subplots(nrows=nrows, ncols=ncols, figsize=(12, 4 * nrows)) # Adjusted figsize and subplots layout\n",
        "fig.suptitle('Model Performance Comparison', y=1.02, fontsize=16)\n",
        "\n",
        "# Flatten the axes array for easy iteration\n",
        "axes = axes.flatten()\n",
        "\n",
        "for i, metric in enumerate(metrics_to_plot):\n",
        "    if metric in eval_df.columns:\n",
        "        eval_df[metric].plot(kind='bar', ax=axes[i], color=sns.color_palette('viridis', len(eval_df)))\n",
        "        axes[i].set_title(f'{metric} Comparison')\n",
        "        axes[i].set_ylabel(metric)\n",
        "        axes[i].tick_params(axis='x', rotation=45)\n",
        "        axes[i].grid(axis='y', linestyle='--', alpha=0.7)\n",
        "    else:\n",
        "        print(f\"Warning: Metric '{metric}' not found in evaluation_metrics DataFrame.\")\n",
        "\n",
        "# Hide any unused subplots\n",
        "for j in range(i + 1, len(axes)):\n",
        "    fig.delaxes(axes[j])\n",
        "\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(\"\\nâœ… Evaluation metrics visualized.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "71c0938c",
      "metadata": {
        "id": "71c0938c"
      },
      "source": [
        "## Model selection\n",
        "Select the best performing model based on the evaluation metrics.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "79952701",
      "metadata": {
        "id": "79952701"
      },
      "outputs": [],
      "source": [
        "# Analyze eval_df and the plots to select the best model.\n",
        "print(\"Analyzing evaluation metrics for model selection...\")\n",
        "display(eval_df)\n",
        "\n",
        "# Identify the best model based on AUC, as it is a good overall metric for imbalanced datasets\n",
        "# and the bar plots show very low Precision, Recall, and F1 for most models.\n",
        "# AUC is less sensitive to class imbalance than other metrics.\n",
        "best_model_name = eval_df['AUC'].idxmax()\n",
        "best_model_auc = eval_df['AUC'].max()\n",
        "\n",
        "print(f\"\\nBased on the evaluation metrics, particularly AUC, the best performing model is: {best_model_name}\")\n",
        "print(f\"Reasoning:\")\n",
        "print(f\"- The target variable is highly imbalanced, as indicated by the low Precision, Recall, and F1-scores for most models.\")\n",
        "print(f\"- AUC (Area Under the ROC Curve) is a robust metric for imbalanced datasets as it measures the model's ability to distinguish between positive and negative classes across various thresholds.\")\n",
        "print(f\"- {best_model_name} achieved the highest AUC score ({best_model_auc:.4f}) among the evaluated models.\")\n",
        "print(f\"- While Accuracy is high for all models (due to class imbalance), other metrics like Precision, Recall, and F1 are very low, highlighting the challenge in predicting the minority class. AUC provides a better indication of overall model performance in this context.\")\n",
        "\n",
        "# Store the best model name in the specified variable\n",
        "# best_model_name is already assigned above\n",
        "print(f\"\\nSelected best model name stored in 'best_model_name' variable.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "23b8ccf5",
      "metadata": {
        "id": "23b8ccf5"
      },
      "source": [
        "## Hyperparameter tuning\n",
        "Tune the hyperparameters of the selected best model to further optimize its performance.\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Use GridSearchCV when compute resources permit to have more thorough search of model tuning space\n",
        "\n",
        "# For now, define a parameter distribution for RandomizedSearchCV\n",
        "# Using distributions instead of a fixed grid\n",
        "# Each factor reduced to fit the compute available. Use larger factors and grids when compute available\n",
        "param_distributions = {\n",
        "    'n_estimators': randint(100, 200), # Reduced max estimators\n",
        "    'learning_rate': uniform(loc=0.01, scale=0.04), # Reduced scale for learning rate\n",
        "    'max_depth': randint(3, 4), # Reduced max depth\n",
        "    'min_samples_split': randint(2, 5), # Reduced max min_samples_split\n",
        "    'min_samples_leaf': randint(1, 3) # Reduced max min_samples_leaf\n",
        "}\n",
        "\n",
        "# Instantiate RandomizedSearchCV\n",
        "# Use 'roc_auc' as the scoring metric (or 'auc')\n",
        "# Set n_iter to control the number of parameter combinations sampled\n",
        "random_search = RandomizedSearchCV(estimator=GradientBoostingClassifier(random_state=42),\n",
        "                                   param_distributions=param_distributions,\n",
        "                                   n_iter=20, # Reduced number of iterations\n",
        "                                   scoring='roc_auc', # Use 'roc_auc' as scoring metric\n",
        "                                   cv=3, # Use 3-fold cross-validation\n",
        "                                   n_jobs=-1, # Used all available cores\n",
        "                                   random_state=42,\n",
        "                                   verbose=2) # Increased verbosity for progress updates\n",
        "\n",
        "# Fit RandomizedSearchCV to the imputed training data\n",
        "print(\"Starting RandomizedSearchCV for hyperparameter tuning (optimizing for AUC)...\")\n",
        "start_time = time.time()\n",
        "\n",
        "# Fit on the imputed training data (X_train_imputed, y_train) - Correcting the data source\n",
        "random_search.fit(X_train_imputed, y_train)\n",
        "\n",
        "end_time = time.time()\n",
        "print(\"RandomizedSearchCV completed.\")\n",
        "print(f\"Total time taken for RandomizedSearchCV: {end_time - start_time:.2f} seconds\")\n",
        "\n",
        "\n",
        "# Get the best parameters and best score\n",
        "best_params = random_search.best_params_\n",
        "best_score = random_search.best_score_\n",
        "\n",
        "print(\"\\nBest parameters found (optimizing for AUC):\")\n",
        "print(best_params)\n",
        "print(f\"\\nBest AUC from cross-validation: {best_score:.4f}\")\n",
        "\n",
        "# Store the best model from the random search\n",
        "best_tuned_model = random_search.best_estimator_\n",
        "print(\"\\nBest tuned model (optimizing for AUC) stored in 'best_tuned_model'.\")\n",
        "\n",
        "# Generate and plot the learning curve for the best tuned model (optimizing for AUC)\n",
        "print(\"\\nGenerating learning curve for the best tuned model (optimizing for AUC)...\")\n",
        "train_sizes, train_scores, test_scores = learning_curve(\n",
        "    best_tuned_model, X_train_imputed, y_train, cv=3, n_jobs=-1, # Use imputed data for learning curve\n",
        "    train_sizes=np.linspace(0.1, 1.0, 3), scoring='roc_auc' # reduced number of train sizes, scoring is 'roc_auc'\n",
        ")\n",
        "\n",
        "train_scores_mean = np.mean(train_scores, axis=1)\n",
        "train_scores_std = np.std(train_scores, axis=1)\n",
        "test_scores_mean = np.mean(test_scores, axis=1)\n",
        "test_scores_std = np.std(test_scores, axis=1)\n",
        "\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.fill_between(train_sizes, train_scores_mean - train_scores_std,\n",
        "                 train_scores_mean + train_scores_std, alpha=0.1, color=\"r\")\n",
        "plt.fill_between(train_sizes, test_scores_mean - test_scores_std,\n",
        "                 test_scores_mean + test_scores_std, alpha=0.1, color=\"g\")\n",
        "plt.plot(train_sizes, train_scores_mean, 'o-', color=\"r\",\n",
        "         label=\"Training score (AUC)\")\n",
        "plt.plot(train_sizes, test_scores_mean, 'o-', color=\"g\",\n",
        "         label=\"Cross-validation score (AUC)\")\n",
        "plt.xlabel(\"Training Examples\")\n",
        "plt.ylabel(\"AUC Score\")\n",
        "plt.title(\"Learning Curve for Best Tuned Gradient Boosting Model (Optimizing for AUC)\")\n",
        "plt.legend(loc=\"best\")\n",
        "plt.grid()\n",
        "plt.show()\n",
        "\n",
        "print(\"\\nâœ… Learning curve generated and displayed (optimizing for AUC).\")"
      ],
      "metadata": {
        "id": "H6z0IDGD4to5"
      },
      "id": "H6z0IDGD4to5",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0115b9d8"
      },
      "source": [
        "## Reducing Overfitting\n",
        "\n",
        "Based on the learning curve, the model shows signs of slight overfitting. Here are some strategies to potentially reduce overfitting in a Gradient Boosting model, which can be added to scope of project when compute and effort permits:\n",
        "\n",
        "*   **Increase Regularization Parameters:**\n",
        "    *   `max_depth`: Decrease the maximum depth of the individual trees. Smaller trees are less likely to capture noise.\n",
        "    *   `min_samples_split`: Increase the minimum number of samples required to split an internal node. This prevents splitting on very small groups of samples.\n",
        "    *   `min_samples_leaf`: Increase the minimum number of samples required to be at a leaf node. This also prevents creating leaves based on very few samples.\n",
        "    *   `subsample`: Introduce randomness by training each tree on a random fraction of the training data (e.g., 0.8 for 80%). This is a form of bagging.\n",
        "    *   `max_features`: Consider a random subset of features when looking for the best split.\n",
        "\n",
        "*   **Increase `n_estimators` (with Early Stopping):** While increasing the number of estimators *can* lead to overfitting if not controlled, using early stopping in conjunction with a larger number of estimators allows the model to train only as long as performance on a validation set is improving.\n",
        "\n",
        "*   **Reduce `learning_rate`:** A smaller learning rate requires more estimators but can lead to a more robust model that generalizes better.\n",
        "\n",
        "*   **More Data:** As observed in the learning curve, increasing the amount of training data often helps reduce overfitting.\n",
        "\n",
        "*   **Feature Selection/Engineering:** Reducing the number of features or creating more meaningful features can sometimes help prevent the model from learning spurious correlations.\n",
        "\n",
        "We can modify the hyperparameter tuning search space and learning curve generation to incorporate some of these strategies to see if we can reduce the observed overfitting, when the effort permits. For now, this will be out of scope of current ietration."
      ],
      "id": "0115b9d8"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2cb694f1"
      },
      "source": [
        "## Evaluate Tuned Model\n",
        "Evaluate the best tuned model on the test set."
      ],
      "id": "2cb694f1"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9598defb"
      },
      "source": [
        "print(\"\\nEvaluating the best tuned model on the test set...\")\n",
        "\n",
        "# Make predictions with the best tuned model\n",
        "y_pred_tuned = best_tuned_model.predict(X_test_imputed)\n",
        "\n",
        "# Calculate evaluation metrics for the tuned model\n",
        "accuracy_tuned = accuracy_score(y_test, y_pred_tuned)\n",
        "precision_tuned = precision_score(y_test, y_pred_tuned)\n",
        "recall_tuned = recall_score(y_test, y_pred_tuned)\n",
        "f1_tuned = f1_score(y_test, y_pred_tuned)\n",
        "\n",
        "# Calculate AUC if the model supports probability predictions\n",
        "auc_tuned = None\n",
        "if hasattr(best_tuned_model, 'predict_proba'):\n",
        "    y_pred_proba_tuned = best_tuned_model.predict_proba(X_test_imputed)[:, 1] # Probability of the positive class\n",
        "    auc_tuned = roc_auc_score(y_test, y_pred_proba_tuned)\n",
        "\n",
        "# Store the tuned model's metrics\n",
        "tuned_model_metrics = {\n",
        "    'Accuracy': accuracy_tuned,\n",
        "    'Precision': precision_tuned,\n",
        "    'Recall': recall_tuned,\n",
        "    'F1-score': f1_tuned\n",
        "}\n",
        "if auc_tuned is not None:\n",
        "    tuned_model_metrics['AUC'] = auc_tuned\n",
        "\n",
        "evaluation_metrics['Tuned_GradientBoosting'] = tuned_model_metrics\n",
        "\n",
        "print(\"âœ… Evaluation of the best tuned model complete.\")\n",
        "print(f\"  Accuracy (Tuned): {accuracy_tuned:.4f}\")\n",
        "print(f\"  Precision (Tuned): {precision_tuned:.4f}\")\n",
        "print(f\"  Recall (Tuned): {recall_tuned:.4f}\")\n",
        "print(f\"  F1-score (Tuned): {f1_tuned:.4f}\")\n",
        "if auc_tuned is not None:\n",
        "    print(f\"  AUC (Tuned): {auc_tuned:.4f}\")"
      ],
      "id": "9598defb",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cad9a0ea"
      },
      "source": [
        "## Visual Metrics (Updated)\n",
        "Visualize the evaluation metrics, before and after the tuning the best model selected before, to compare performance."
      ],
      "id": "cad9a0ea"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b7ea7926"
      },
      "source": [
        "# 1. Convert the updated evaluation_metrics dictionary into a pandas DataFrame\n",
        "eval_df_tuned = pd.DataFrame(evaluation_metrics).T # Transpose to have models as rows\n",
        "\n",
        "# Filter to include only the best model before tuning and the best tuned model\n",
        "# Assuming 'best_model_name' stores the name of the best model before tuning\n",
        "# and 'Tuned_GradientBoosting' is the name used for the best model after tuning.\n",
        "models_to_compare = [best_model_name, 'Tuned_GradientBoosting']\n",
        "eval_df_comparison = eval_df_tuned.loc[models_to_compare]\n",
        "\n",
        "\n",
        "# 2. Create bar plots for each metric\n",
        "metrics_to_plot = ['Accuracy', 'AUC', 'F1-score', 'Precision', 'Recall']\n",
        "\n",
        "print(\"Generating updated plots for evaluation metrics (Tuned vs. Before Tuning)...\")\n",
        "\n",
        "# Calculate the number of rows needed (ceil(num_metrics / 2))\n",
        "n_metrics = len(metrics_to_plot)\n",
        "nrows = (n_metrics + 1) // 2\n",
        "ncols = 2 # Two plots per line\n",
        "\n",
        "fig, axes = plt.subplots(nrows=nrows, ncols=ncols, figsize=(12, 4 * nrows)) # Adjusted figsize and subplots layout\n",
        "fig.suptitle('Model Performance Comparison (Tuned vs. Before Tuning)', y=1.02, fontsize=16)\n",
        "\n",
        "# Flatten the axes array for easy iteration\n",
        "axes = axes.flatten()\n",
        "\n",
        "for i, metric in enumerate(metrics_to_plot):\n",
        "    if metric in eval_df_comparison.columns:\n",
        "        eval_df_comparison[metric].plot(kind='bar', ax=axes[i], color=sns.color_palette('viridis', len(eval_df_comparison)))\n",
        "        axes[i].set_title(f'{metric} Comparison')\n",
        "        axes[i].set_ylabel(metric)\n",
        "        axes[i].tick_params(axis='x', rotation=45)\n",
        "        axes[i].grid(axis='y', linestyle='--', alpha=0.7)\n",
        "    else:\n",
        "        print(f\"Warning: Metric '{metric}' not found in evaluation_metrics DataFrame.\")\n",
        "\n",
        "# Hide any unused subplots\n",
        "for j in range(i + 1, len(axes)):\n",
        "    fig.delaxes(axes[j])\n",
        "\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(\"\\nâœ… Updated evaluation metrics visualized.\")"
      ],
      "id": "b7ea7926",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8f06a5f3"
      },
      "source": [
        "### Conclusion: Impact of Hyperparameter Tuning\n",
        "\n",
        "Based on the comparison of the Gradient Boosting model's performance before and after hyperparameter tuning (as shown in the table and graphs above):\n",
        "\n",
        "*   **Accuracy:** Accuracy remained very high (around 0.994) both before and after tuning. However, due to the severe class imbalance, Accuracy is not the most informative metric for this problem.\n",
        "*   **Precision:** Precision saw a significant increase after tuning (from 0.0909 to 0.3333). This indicates that when the tuned model predicts a positive case, it is more likely to be correct compared to the untuned model. This is a positive outcome of tuning.\n",
        "*   **Recall:** Recall remained low (around 0.0667) both before and after tuning. This suggests that even with tuning, the model still struggles to identify a large proportion of the actual positive cases.\n",
        "*   **F1-score:** The F1-score, which is the harmonic mean of Precision and Recall, also saw an improvement after tuning (from 0.0769 to 0.1111). This reflects the gain in Precision, although the low Recall still keeps the F1-score relatively low.\n",
        "*   **AUC:** The AUC score saw a slight increase after tuning (from 0.6357 to 0.6368). While the improvement is marginal, AUC is a robust metric for imbalanced data and shows a small positive impact of tuning on the model's ability to distinguish between the two classes.\n",
        "\n",
        "**Overall Conclusion:**\n",
        "\n",
        "Hyperparameter tuning of the Gradient Boosting model resulted in a noticeable improvement in **Precision** and a slight increase in **AUC**. While **Recall** remains a challenge, the tuned model is better at avoiding false positives when it does predict a positive case. This suggests that the tuning process helped to refine the model's decision boundaries to be more accurate for the minority class, although the difficulty in identifying all positive cases (Recall) persists due to the dataset's imbalance and potentially the inherent complexity of the problem."
      ],
      "id": "8f06a5f3"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8c2fd8e9"
      },
      "source": [
        "# Improvement on Sampling method\n",
        "Implement and compare oversampling (SMOTE) and undersampling techniques on the training data, train the best-tuned model on the resampled data, evaluate their performance on the original test set, and determine the best resampling strategy for the Bosch dataset."
      ],
      "id": "8c2fd8e9"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f384d574"
      },
      "source": [
        "## Implement oversampling (SMOTE)\n",
        "Apply the SMOTE technique to the training data to create synthetic samples for the minority class.\n"
      ],
      "id": "f384d574"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b67d60b1"
      },
      "source": [
        "# 1. Import the SMOTE class (if not already imported)\n",
        "from imblearn.over_sampling import SMOTE\n",
        "\n",
        "# 2. Instantiate a SMOTE object\n",
        "smote = SMOTE(random_state=42)\n",
        "\n",
        "# 3. Apply SMOTE to the imputed training data\n",
        "print(\"Applying SMOTE to the training data...\")\n",
        "X_train_resampled_smote, y_train_resampled_smote = smote.fit_resample(X_train_imputed, y_train)\n",
        "print(\"SMOTE application complete.\")\n",
        "\n",
        "# 4. Store the oversampled features and target (already done in step 3)\n",
        "\n",
        "print(f\"Original training data shape: {X_train_imputed.shape}\")\n",
        "print(f\"Oversampled training data shape (SMOTE): {X_train_resampled_smote.shape}\")\n",
        "print(f\"Original target distribution:\\n{y_train.value_counts()}\")\n",
        "print(f\"Oversampled target distribution (SMOTE):\\n{y_train_resampled_smote.value_counts()}\")\n"
      ],
      "id": "b67d60b1",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5c95906b"
      },
      "source": [
        "## Train and evaluate model with oversampling\n",
        "Train the best-tuned model on the oversampled training data and evaluate its performance on the original test set.\n"
      ],
      "id": "5c95906b"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fffc0629"
      },
      "source": [
        "print(\"\\nTraining the best tuned model on SMOTE oversampled training data...\")\n",
        "\n",
        "# Train the best_tuned_model using the oversampled training data\n",
        "best_tuned_model_smote = best_tuned_model # Start with the best tuned model structure\n",
        "best_tuned_model_smote.fit(X_train_resampled_smote, y_train_resampled_smote)\n",
        "\n",
        "print(\"âœ… Training on SMOTE oversampled data complete.\")\n",
        "\n",
        "# Make predictions on the original imputed test set\n",
        "print(\"\\nEvaluating the model trained on SMOTE data on the test set...\")\n",
        "y_pred_tuned_smote = best_tuned_model_smote.predict(X_test_imputed)\n",
        "\n",
        "# Calculate evaluation metrics\n",
        "accuracy_tuned_smote = accuracy_score(y_test, y_pred_tuned_smote)\n",
        "precision_tuned_smote = precision_score(y_test, y_pred_tuned_smote)\n",
        "recall_tuned_smote = recall_score(y_test, y_pred_tuned_smote)\n",
        "f1_tuned_smote = f1_score(y_test, y_pred_tuned_smote)\n",
        "\n",
        "# Calculate AUC if the model supports probability predictions\n",
        "auc_tuned_smote = None\n",
        "if hasattr(best_tuned_model_smote, 'predict_proba'):\n",
        "    y_pred_proba_tuned_smote = best_tuned_model_smote.predict_proba(X_test_imputed)[:, 1] # Probability of the positive class\n",
        "    auc_tuned_smote = roc_auc_score(y_test, y_pred_proba_tuned_smote)\n",
        "\n",
        "# Store the tuned model's metrics\n",
        "tuned_model_smote_metrics = {\n",
        "    'Accuracy': accuracy_tuned_smote,\n",
        "    'Precision': precision_tuned_smote,\n",
        "    'Recall': recall_tuned_smote,\n",
        "    'F1-score': f1_tuned_smote\n",
        "}\n",
        "if auc_tuned_smote is not None:\n",
        "    tuned_model_smote_metrics['AUC'] = auc_tuned_smote\n",
        "\n",
        "evaluation_metrics['Tuned_GradientBoosting_SMOTE'] = tuned_model_smote_metrics\n",
        "\n",
        "print(\"âœ… Evaluation complete for model trained on SMOTE data.\")\n",
        "print(f\"  Accuracy (Tuned + SMOTE): {accuracy_tuned_smote:.4f}\")\n",
        "print(f\"  Precision (Tuned + SMOTE): {precision_tuned_smote:.4f}\")\n",
        "print(f\"  Recall (Tuned + SMOTE): {recall_tuned_smote:.4f}\")\n",
        "print(f\"  F1-score (Tuned + SMOTE): {f1_tuned_smote:.4f}\")\n",
        "if auc_tuned_smote is not None:\n",
        "    print(f\"  AUC (Tuned + SMOTE): {auc_tuned_smote:.4f}\")"
      ],
      "id": "fffc0629",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "26bd5939"
      },
      "source": [
        "## Implement undersampling\n",
        "Apply an undersampling technique (e.g., RandomUnderSampler) to the training data to reduce the number of samples in the majority class.\n"
      ],
      "id": "26bd5939"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c38a6d59"
      },
      "source": [
        "# Instantiate a RandomUnderSampler object\n",
        "rus = RandomUnderSampler(random_state=42)\n",
        "\n",
        "# Apply the undersampling technique to the imputed training data\n",
        "print(\"Applying RandomUnderSampler to the training data...\")\n",
        "X_train_resampled_rus, y_train_resampled_rus = rus.fit_resample(X_train_imputed, y_train)\n",
        "print(\"RandomUnderSampler application complete.\")\n",
        "\n",
        "# Print the original and undersampled training data shapes and target distributions\n",
        "print(f\"\\nOriginal training data shape: {X_train_imputed.shape}\")\n",
        "print(f\"Undersampled training data shape (RandomUnderSampler): {X_train_resampled_rus.shape}\")\n",
        "print(f\"\\nOriginal target distribution:\\n{y_train.value_counts()}\")\n",
        "print(f\"\\nUndersampled target distribution (RandomUnderSampler):\\n{y_train_resampled_rus.value_counts()}\")"
      ],
      "id": "c38a6d59",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "311a5ad4"
      },
      "source": [
        "print(\"\\nTraining the best tuned model on RandomUnderSampler undersampled training data...\")\n",
        "\n",
        "# Train the best_tuned_model using the undersampled training data\n",
        "best_tuned_model_rus = best_tuned_model # Start with the best tuned model structure\n",
        "best_tuned_model_rus.fit(X_train_resampled_rus, y_train_resampled_rus)\n",
        "\n",
        "print(\"âœ… Training on RandomUnderSampler undersampled data complete.\")\n",
        "\n",
        "# Make predictions on the original imputed test set\n",
        "print(\"\\nEvaluating the model trained on RandomUnderSampler data on the test set...\")\n",
        "y_pred_tuned_rus = best_tuned_model_rus.predict(X_test_imputed)\n",
        "\n",
        "# Calculate evaluation metrics\n",
        "accuracy_tuned_rus = accuracy_score(y_test, y_pred_tuned_rus)\n",
        "precision_tuned_rus = precision_score(y_test, y_pred_tuned_rus)\n",
        "recall_tuned_rus = recall_score(y_test, y_pred_tuned_rus)\n",
        "f1_tuned_rus = f1_score(y_test, y_pred_tuned_rus)\n",
        "\n",
        "# Calculate AUC if the model supports probability predictions\n",
        "auc_tuned_rus = None\n",
        "if hasattr(best_tuned_model_rus, 'predict_proba'):\n",
        "    y_pred_proba_tuned_rus = best_tuned_model_rus.predict_proba(X_test_imputed)[:, 1] # Probability of the positive class\n",
        "    auc_tuned_rus = roc_auc_score(y_test, y_pred_proba_tuned_rus)\n",
        "\n",
        "# Store the tuned model's metrics\n",
        "tuned_model_rus_metrics = {\n",
        "    'Accuracy': accuracy_tuned_rus,\n",
        "    'Precision': precision_tuned_rus,\n",
        "    'Recall': recall_tuned_rus,\n",
        "    'F1-score': f1_tuned_rus\n",
        "}\n",
        "if auc_tuned_rus is not None:\n",
        "    tuned_model_rus_metrics['AUC'] = auc_tuned_rus\n",
        "\n",
        "evaluation_metrics['Tuned_GradientBoosting_RUS'] = tuned_model_rus_metrics\n",
        "\n",
        "print(\"âœ… Evaluation complete for model trained on RandomUnderSampler data.\")\n",
        "print(f\"  Accuracy (Tuned + RUS): {accuracy_tuned_rus:.4f}\")\n",
        "print(f\"  Precision (Tuned + RUS): {precision_tuned_rus:.4f}\")\n",
        "print(f\"  Recall (Tuned + RUS): {recall_tuned_rus:.4f}\")\n",
        "print(f\"  F1-score (Tuned + RUS): {f1_tuned_rus:.4f}\")\n",
        "if auc_tuned_rus is not None:\n",
        "    print(f\"  AUC (Tuned + RUS): {auc_tuned_rus:.4f}\")\n"
      ],
      "id": "311a5ad4",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2dae9975"
      },
      "source": [
        "## Compare resampling techniques\n",
        "Compare the evaluation metrics obtained from the models trained with oversampling and undersampling to determine which technique is more effective for this dataset.\n"
      ],
      "id": "2dae9975"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4e1e8069"
      },
      "source": [
        "# Convert the evaluation_metrics dictionary into a pandas DataFrame\n",
        "evaluation_df = pd.DataFrame(evaluation_metrics).T\n",
        "\n",
        "# Display the DataFrame for comparison\n",
        "print(\"Comparison of Model Performance with Resampling Techniques:\")\n",
        "display(evaluation_df)\n",
        "\n",
        "# Analyze the metrics\n",
        "print(\"\\nAnalysis of Metrics:\")\n",
        "print(\"- The 'Tuned_GradientBoosting' model is the baseline after initial tuning without resampling.\")\n",
        "print(\"- 'Tuned_GradientBoosting_SMOTE' shows performance after applying SMOTE oversampling.\")\n",
        "print(\"- 'Tuned_GradientBoosting_RUS' shows performance after applying RandomUnderSampler.\")\n",
        "\n",
        "print(\"\\nKey observations based on metrics (Accuracy, Precision, Recall, F1-score, AUC):\")\n",
        "print(\"- **Accuracy:** High for models trained on original and SMOTE data, lower for RUS. This is expected due to class imbalance; high accuracy on imbalanced data can be misleading if the model simply predicts the majority class.\")\n",
        "print(\"- **Precision, Recall, F1-score:** Generally low across all models, indicating difficulty in correctly identifying the minority class (Response=1). RUS shows a higher Recall but significantly lower Precision and F1-score compared to SMOTE and the original tuned model, suggesting it identifies more positive cases but at the cost of many false positives.\")\n",
        "print(\"- **AUC:** This is a better metric for imbalanced datasets. The AUC scores are relatively similar across the tuned models (original, SMOTE, RUS), ranging from approximately 0.60 to 0.63. The original tuned model and the SMOTE-trained model have slightly higher AUCs compared to the RUS-trained model on the test set.\")\n",
        "\n",
        "print(\"\\nBased on this comparison, SMOTE appears to be a slightly more effective resampling technique than RandomUnderSampler for this dataset when considering the balance between identifying positive cases (Recall) and minimizing false positives (Precision), while maintaining a competitive AUC.\")\n"
      ],
      "id": "4e1e8069",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ea73d434"
      },
      "source": [
        "## More Discussion : Sampling Method - OverSampling vs UnderSampling\n",
        "\n",
        "*   Applying SMOTE oversampling successfully balanced the training data distribution, increasing the number of minority class samples from 61 to 10424, matching the majority class. The total training data size increased from 10485 to 20848 samples.\n",
        "*   Training the tuned Gradient Boosting model on SMOTE data resulted in test set metrics: Accuracy 0.9920, Precision 0.1250, Recall 0.0667, F1-score 0.0870, and AUC 0.6241.\n",
        "*   Applying RandomUnderSampler undersampling reduced the majority class samples from 10424 to 61, balancing the distribution. The total training data size was significantly reduced from 10485 to 122 samples.\n",
        "*   Training the tuned Gradient Boosting model on RandomUnderSampler data resulted in test set metrics: Accuracy 0.6159, Precision 0.0079, Recall 0.5333, F1-score 0.0156, and AUC 0.6062.\n",
        "*   Comparing the performance on the original test set, the model trained with SMOTE oversampling showed better overall metrics (Precision, F1-score, and AUC) compared to the model trained with RandomUnderSampler, despite the latter having a much higher Recall.\n",
        "*   The AUC scores, which are less sensitive to class imbalance, were similar across the tuned models (original, SMOTE, RUS), ranging from approximately 0.60 to 0.63, with SMOTE having a slightly higher AUC than RUS.\n",
        "\n",
        "### Insights or Next Steps\n",
        "\n",
        "*   SMOTE appears to be a more effective resampling strategy than RandomUnderSampler for this dataset, offering a better trade-off between identifying positive cases (Recall) and minimizing false positives (Precision) while maintaining a competitive AUC.\n",
        "*   Given the continued low Precision and F1-score even with SMOTE, further investigation into more advanced resampling techniques or exploring ensemble methods specifically designed for imbalanced data might be beneficial to improve the model's ability to correctly identify the minority class.\n"
      ],
      "id": "ea73d434"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "50eed215"
      },
      "source": [
        "## Explore advanced resampling\n",
        "Implement and evaluate more advanced oversampling techniques (e.g., ADASYN, Borderline-SMOTE) or a combination of oversampling and undersampling (e.g., SMOTE-ENN, SMOTE-Tomek).\n"
      ],
      "id": "50eed215"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "496a587c"
      },
      "source": [
        "# Instantiate each advanced resampling technique\n",
        "adasyn = ADASYN(random_state=42)\n",
        "borderline_smote = BorderlineSMOTE(random_state=42)\n",
        "smote_enn = SMOTEENN(random_state=42)\n",
        "smote_tomek = SMOTETomek(random_state=42)\n",
        "\n",
        "resamplers = {\n",
        "    'ADASYN': adasyn,\n",
        "    'BorderlineSMOTE': borderline_smote,\n",
        "    'SMOTEENN': smote_enn,\n",
        "    'SMOTETomek': smote_tomek\n",
        "}\n",
        "\n",
        "# Apply resampling, train model, evaluate, and store metrics\n",
        "print(\"\\nExploring advanced resampling techniques...\")\n",
        "\n",
        "for name, resampler in resamplers.items():\n",
        "    print(f\"\\nApplying {name}...\")\n",
        "    X_train_resampled, y_train_resampled = resampler.fit_resample(X_train_imputed, y_train)\n",
        "    print(f\"{name} application complete. Resampled shape: {X_train_resampled.shape}\")\n",
        "    print(f\"Resampled target distribution:\\n{y_train_resampled.value_counts()}\")\n",
        "\n",
        "    print(f\"Training best tuned model on {name} resampled data...\")\n",
        "    model_resampled = best_tuned_model # Start with the best tuned model structure\n",
        "    model_resampled.fit(X_train_resampled, y_train_resampled)\n",
        "    print(\"Training complete.\")\n",
        "\n",
        "    print(f\"Evaluating model trained on {name} data on the test set...\")\n",
        "    y_pred_resampled = model_resampled.predict(X_test_imputed)\n",
        "\n",
        "    # Calculate evaluation metrics\n",
        "    accuracy_resampled = accuracy_score(y_test, y_pred_resampled)\n",
        "    precision_resampled = precision_score(y_test, y_pred_resampled)\n",
        "    recall_resampled = recall_score(y_test, y_pred_resampled)\n",
        "    f1_resampled = f1_score(y_test, y_pred_resampled)\n",
        "\n",
        "    # Calculate AUC if the model supports probability predictions\n",
        "    auc_resampled = None\n",
        "    if hasattr(model_resampled, 'predict_proba'):\n",
        "        y_pred_proba_resampled = model_resampled.predict_proba(X_test_imputed)[:, 1] # Probability of the positive class\n",
        "        auc_resampled = roc_auc_score(y_test, y_pred_proba_resampled)\n",
        "\n",
        "    # Store the model's metrics\n",
        "    resampled_model_metrics = {\n",
        "        'Accuracy': accuracy_resampled,\n",
        "        'Precision': precision_resampled,\n",
        "        'Recall': recall_resampled,\n",
        "        'F1-score': f1_resampled\n",
        "    }\n",
        "    if auc_resampled is not None:\n",
        "        resampled_model_metrics['AUC'] = auc_resampled\n",
        "\n",
        "    evaluation_metrics[f'Tuned_GradientBoosting_{name}'] = resampled_model_metrics\n",
        "\n",
        "    # 7. Print the evaluation metrics\n",
        "    print(f\"âœ… Evaluation complete for model trained on {name} data.\")\n",
        "    print(f\"  Accuracy (Tuned + {name}): {accuracy_resampled:.4f}\")\n",
        "    print(f\"  Precision (Tuned + {name}): {precision_resampled:.4f}\")\n",
        "    print(f\"  Recall (Tuned + {name}): {recall_resampled:.4f}\")\n",
        "    print(f\"  F1-score (Tuned + {name}): {f1_resampled:.4f}\")\n",
        "    if auc_resampled is not None:\n",
        "        print(f\"  AUC (Tuned + {name}): {auc_resampled:.4f}\")\n",
        "\n",
        "print(\"\\nâœ… Evaluation of advanced resampling techniques complete.\")"
      ],
      "id": "496a587c",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2e71543b"
      },
      "source": [
        "## Explore ensemble methods for imbalanced data\n",
        "Train and evaluate ensemble methods specifically designed for imbalanced datasets (e.g., BalancedBaggingClassifier, EasyEnsembleClassifier, BalancedRandomForestClassifier).\n"
      ],
      "id": "2e71543b"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9e3a0116"
      },
      "source": [
        "# Instantiate each of the selected imbalanced ensemble classifiers\n",
        "# Create a dictionary to store these imbalanced ensemble classifiers\n",
        "imbalanced_ensemble_models = {\n",
        "    'BalancedBaggingClassifier': BalancedBaggingClassifier(random_state=42),\n",
        "    'EasyEnsembleClassifier': EasyEnsembleClassifier(random_state=42),\n",
        "    'BalancedRandomForestClassifier': BalancedRandomForestClassifier(random_state=42)\n",
        "}\n",
        "\n",
        "# Iterate through the imbalanced ensemble classifiers and train/evaluate\n",
        "print(\"\\nTraining and evaluating imbalanced ensemble methods...\")\n",
        "\n",
        "for name, model in imbalanced_ensemble_models.items():\n",
        "    print(f\"\\nTraining {name}...\")\n",
        "    # Train the classifier on the imputed training data\n",
        "    # These methods handle the imbalance internally\n",
        "    model.fit(X_train_imputed, y_train)\n",
        "    print(f\"  {name} training complete.\")\n",
        "\n",
        "    # Make predictions on the imputed test data\n",
        "    print(f\"  Evaluating {name} on the test set...\")\n",
        "    y_pred = model.predict(X_test_imputed)\n",
        "\n",
        "    # Calculate evaluation metrics\n",
        "    accuracy = accuracy_score(y_test, y_pred)\n",
        "    precision = precision_score(y_test, y_pred)\n",
        "    recall = recall_score(y_test, y_pred)\n",
        "    f1 = f1_score(y_test, y_pred)\n",
        "\n",
        "    # Calculate AUC if the model supports probability predictions\n",
        "    auc = None\n",
        "    if hasattr(model, 'predict_proba'):\n",
        "        y_pred_proba = model.predict_proba(X_test_imputed)[:, 1] # Probability of the positive class\n",
        "        auc = roc_auc_score(y_test, y_pred_proba)\n",
        "\n",
        "    # Store the calculated metrics\n",
        "    model_metrics = {\n",
        "        'Accuracy': accuracy,\n",
        "        'Precision': precision,\n",
        "        'Recall': recall,\n",
        "        'F1-score': f1\n",
        "    }\n",
        "    if auc is not None:\n",
        "        model_metrics['AUC'] = auc\n",
        "\n",
        "    evaluation_metrics[name] = model_metrics\n",
        "\n",
        "    # Print the evaluation metrics\n",
        "    print(f\"  âœ… Evaluation complete for {name}.\")\n",
        "    print(f\"    Accuracy: {accuracy:.4f}\")\n",
        "    print(f\"    Precision: {precision:.4f}\")\n",
        "    print(f\"    Recall: {recall:.4f}\")\n",
        "    print(f\"    F1-score: {f1:.4f}\")\n",
        "    if auc is not None:\n",
        "        print(f\"    AUC: {auc:.4f}\")\n",
        "\n",
        "# Print a confirmation message\n",
        "print(\"\\nâœ… Evaluation of imbalanced ensemble methods complete.\")"
      ],
      "id": "9e3a0116",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4ac07481"
      },
      "source": [
        "## Compare Advanced Resampling and Ensemble Methods\n",
        "Compare the evaluation metrics obtained from the models trained with advanced resampling and imbalanced ensemble methods to determine the most effective strategy for this dataset."
      ],
      "id": "4ac07481"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "21f946fb"
      },
      "source": [
        "# Convert the evaluation_metrics dictionary into a pandas DataFrame for comprehensive comparison\n",
        "evaluation_df_all = pd.DataFrame(evaluation_metrics).T\n",
        "\n",
        "# Display the DataFrame, sorted by AUC for easier comparison of performance on imbalanced data\n",
        "print(\"Comprehensive Comparison of Model Performance with Various Techniques:\")\n",
        "display(evaluation_df_all.sort_values(by='AUC', ascending=False))\n",
        "\n",
        "# Provide an analysis of the results\n",
        "print(\"\\nAnalysis of Comprehensive Model Performance:\")\n",
        "print(\"- This table shows the performance metrics for the baseline model, the initially tuned model, models trained with different resampling techniques (SMOTE, RUS, ADASYN, BorderlineSMOTE, SMOTEENN, SMOTETomek), and imbalanced ensemble methods (BalancedBaggingClassifier, EasyEnsembleClassifier, BalancedRandomForestClassifier).\")\n",
        "print(\"- The metrics are sorted by AUC in descending order, as AUC is a suitable metric for comparing models on imbalanced datasets.\")\n",
        "\n",
        "print(\"\\nKey Findings:\")\n",
        "\n",
        "print(\"\\nDetermining the Best Strategy:\")\n",
        "\n",
        "best_overall_model_name = evaluation_df_all['AUC'].idxmax()\n",
        "best_overall_model_auc = evaluation_df_all['AUC'].max()\n",
        "\n",
        "print(f\"\\nBased on the comprehensive evaluation of all techniques, the best performing approach based on AUC is: {best_overall_model_name}\")\n",
        "print(f\"It achieved the highest AUC score of {best_overall_model_auc:.4f}.\")\n",
        "print(\"\\nReasoning for selecting based on AUC:\")\n",
        "print(\"- AUC is a robust metric for imbalanced datasets, as it measures the model's ability to discriminate between positive and negative classes across various thresholds.\")\n",
        "print(\"- While other metrics like Precision, Recall, and F1-score are important, they are highly sensitive to the classification threshold. AUC provides an overall measure of the model's performance regardless of the threshold.\")\n",
        "print(\"- In this analysis, we prioritized AUC as the primary metric for selecting the best overall modeling approach.\")\n",
        "\n",
        "# Note: While AUC is used for initial ranking, the 'best' strategy should also consider the application's specific needs regarding Precision and Recall when making deployment decisions.\n",
        "# Precision recall will be analysed for optimization and optimized if required, later."
      ],
      "id": "21f946fb",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d034d22f"
      },
      "source": [
        "## Visual Metrics (All Models)\n",
        "Visualize the evaluation metrics for all models to compare their performance."
      ],
      "id": "d034d22f"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eb25e461"
      },
      "source": [
        "# 1. Convert the evaluation_metrics dictionary into a pandas DataFrame\n",
        "eval_df_all = pd.DataFrame(evaluation_metrics).T\n",
        "\n",
        "# Filter to include the best tuned model and all advanced resampling/imbalanced ensemble models\n",
        "models_to_compare_advanced = ['Tuned_GradientBoosting'] + [\n",
        "    name for name in evaluation_metrics.keys()\n",
        "    if name.startswith('Tuned_GradientBoosting_') or name in imbalanced_ensemble_models.keys()\n",
        "]\n",
        "\n",
        "# Ensure unique model names and maintain order\n",
        "models_to_compare_advanced = list(dict.fromkeys(models_to_compare_advanced))\n",
        "\n",
        "# Create a comparison DataFrame\n",
        "eval_df_advanced_comparison = eval_df_all.loc[models_to_compare_advanced]\n",
        "\n",
        "\n",
        "# 2. Create bar plots for each metric\n",
        "metrics_to_plot = ['Accuracy', 'AUC', 'F1-score', 'Precision', 'Recall']\n",
        "\n",
        "print(\"Generating plots comparing Best Tuned Model with Advanced Resampling and Ensemble Methods...\")\n",
        "\n",
        "# Calculate the number of rows needed (one row per metric)\n",
        "n_metrics = len(metrics_to_plot)\n",
        "nrows = n_metrics\n",
        "ncols = 1 # One plot per line\n",
        "\n",
        "fig, axes = plt.subplots(nrows=nrows, ncols=ncols, figsize=(10, 6 * nrows)) # Adjusted figsize and subplots layout\n",
        "fig.suptitle('Model Performance Comparison (Tuned GB vs. Advanced Techniques)', y=1.02, fontsize=16)\n",
        "\n",
        "# Flatten the axes array for easy iteration if needed (not strictly necessary with ncols=1)\n",
        "if nrows > 1:\n",
        "    axes = axes.flatten()\n",
        "else:\n",
        "    axes = [axes]\n",
        "\n",
        "\n",
        "for i, metric in enumerate(metrics_to_plot):\n",
        "    if metric in eval_df_advanced_comparison.columns:\n",
        "        eval_df_advanced_comparison[metric].sort_values(ascending=False).plot(kind='bar', ax=axes[i], color=sns.color_palette('viridis', len(eval_df_advanced_comparison)))\n",
        "        axes[i].set_title(f'{metric} Comparison')\n",
        "        axes[i].set_ylabel(metric)\n",
        "        axes[i].tick_params(axis='x', rotation=90) # Rotate labels for better readability with many models\n",
        "        axes[i].grid(axis='y', linestyle='--', alpha=0.7)\n",
        "    else:\n",
        "        print(f\"Warning: Metric '{metric}' not found in evaluation_metrics DataFrame.\")\n",
        "\n",
        "# Hide any unused subplots (not needed with ncols=1 and nrows = n_metrics)\n",
        "\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(\"\\nâœ… Advanced evaluation metrics visualized.\")"
      ],
      "id": "eb25e461",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7eeae3e1"
      },
      "source": [
        "## Model Selection Justification\n",
        "Detailed justification for selecting the final model based on the evaluation metrics and problem requirements."
      ],
      "id": "7eeae3e1"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2a0c4ccd"
      },
      "source": [
        "## Model Selection\n",
        "\n",
        "Based on the comprehensive evaluation of various models and resampling techniques, the **Gradient Boosting Classifier with ADASYN Resampling** is selected as the final model for predicting product failures. Here is the detailed justification:\n",
        "\n",
        "1.  **Objective of the Problem:** The core objective is to accurately predict product failures in a high-dimensional manufacturing process. This requires a model that can effectively handle the class imbalance inherent in manufacturing failure data and provide reliable predictions of the minority class (failures).\n",
        "\n",
        "2.  **Evaluation Metrics:** Given the imbalanced nature of the dataset (very few failures compared to non-failures), metrics like Accuracy can be misleading. Instead, metrics that specifically assess performance on the minority class and the model's ability to distinguish between classes are crucial.\n",
        "    *   **AUC (Area Under the ROC Curve):** This metric measures the model's ability to discriminate between positive and negative classes across all possible classification thresholds. It is robust to class imbalance and provides a good overall measure of model performance. The **Gradient Boosting Classifier with ADASYN Resampling** achieved the highest AUC (0.6522) among all models evaluated, indicating superior discriminative power.\n",
        "    *   **Precision:** This metric measures the accuracy of the positive predictions (i.e., out of all predicted failures, how many were actual failures). A higher precision is desirable to minimize false positives, which can lead to unnecessary inspections or interventions. While still relatively low, the tuned Gradient Boosting model *without* resampling showed the highest precision (0.3333). However, this came at the cost of very low recall.\n",
        "    *   **Recall:** This metric measures the model's ability to find all the positive cases (i.e., out of all actual failures, how many were correctly identified). A higher recall is important to minimize false negatives, which means failing to detect a faulty product. Undersampling (RUS) showed a high recall (0.4667), but with extremely low precision and AUC.\n",
        "    *   **F1-score:** This metric is the harmonic mean of Precision and Recall, providing a single score that balances both. It is a good indicator of a model's performance on the minority class.\n",
        "\n",
        "3.  **Handling Class Imbalance:** The severe class imbalance is a major challenge. Techniques like oversampling (SMOTE, ADASYN, BorderlineSMOTE), combined resampling (SMOTE-ENN, SMOTE-Tomek), and imbalanced ensemble methods (BalancedBaggingClassifier, EasyEnsembleClassifier, BalancedRandomForestClassifier) were explored to address this.\n",
        "    *   **ADASYN (Adaptive Synthetic Sampling):** This technique is similar to SMOTE but focuses on generating synthetic samples for minority class instances that are harder to learn (near the decision boundary). The evaluation showed that training the tuned Gradient Boosting model on ADASYN resampled data resulted in the highest AUC, suggesting that ADASYN helped the model better distinguish between the classes in this specific dataset compared to other resampling methods.\n",
        "    *   While some imbalanced ensemble methods (like EasyEnsembleClassifier) achieved competitive AUCs, they often resulted in extremely low precision, making them less practical for a manufacturing setting where minimizing false positives is also important.\n",
        "\n",
        "4.  **Model Interpretability:** Gradient Boosting models, while powerful, can be less directly interpretable than simpler models like Logistic Regression or Decision Trees. However, techniques like feature importance (which can be derived from the trained model) and permutation importance can still provide insights into which features are most influential in the predictions. This aligns with the problem requirement for interpretable insights.\n",
        "\n",
        "**Conclusion:**\n",
        "\n",
        "The **Gradient Boosting Classifier with ADASYN Resampling** strikes the best balance among the evaluated approaches for this problem. It demonstrates the highest discriminative power (AUC) on the test set, indicating its superior ability to rank failure instances higher than non-failure instances. While Precision and Recall remain challenging due to the dataset's nature, the combination of a tuned Gradient Boosting model and ADASYN resampling provides the most promising performance profile according to the chosen evaluation metrics for an imbalanced classification task. Future work can explore further tuning of this specific model and resampling technique, as well as delving deeper into feature importance analysis for interpretability."
      ],
      "id": "2a0c4ccd"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0fedb8f3"
      },
      "source": [
        "## Implement and Visualize Final Model Predictions\n",
        "Implement the final selected model (Gradient Boosting with ADASYN) on the test set and visualize its predictions to demonstrate its performance."
      ],
      "id": "0fedb8f3"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7eae6122"
      },
      "source": [
        "print(\"Implementing and visualizing predictions of the final model (Gradient Boosting with ADASYN) on the test set...\")\n",
        "\n",
        "# Retrieve the best performing model (Gradient Boosting with ADASYN)\n",
        "# Based on the previous analysis, the model trained on ADASYN data had the highest AUC.\n",
        "# The model object itself is the best_tuned_model trained on X_train_resampled_adasyn, y_train_resampled_adasyn\n",
        "# We need to retrain this model explicitly here if we don't have the resampled data readily available from the best model object\n",
        "\n",
        "# We already have the best_tuned_model object which contains the structure and hyperparameters\n",
        "# We need to get the resampled data used for the 'Tuned_GradientBoosting_ADASYN' training\n",
        "# X_train_resampled_adasyn and y_train_resampled_adasyn\n",
        "\n",
        "# Find the ADASYN resampler again\n",
        "adasyn = ADASYN(random_state=42)\n",
        "\n",
        "# Apply ADASYN to the imputed training data to get the resampled data used for training\n",
        "# Note: This step is done again to ensure we have the correct resampled data if it wasn't stored globally\n",
        "print(\"Applying ADASYN resampling to the imputed training data for final model implementation...\")\n",
        "X_train_resampled_adasyn, y_train_resampled_adasyn = adasyn.fit_resample(X_train_imputed, y_train)\n",
        "print(\"ADASYN resampling complete.\")\n",
        "\n",
        "# Train a *new* instance of the best_tuned_model on the ADASYN resampled data\n",
        "# This ensures we use the correct model trained on the specific data for reporting\n",
        "final_model = GradientBoostingClassifier(**best_params, random_state=42) # Use the best hyperparameters\n",
        "final_model.fit(X_train_resampled_adasyn, y_train_resampled_adasyn)\n",
        "print(\"Final model (Gradient Boosting with ADASYN) trained on resampled data.\")\n",
        "\n",
        "\n",
        "# Make predictions on the original imputed test set\n",
        "y_pred_final = final_model.predict(X_test_imputed)\n",
        "y_pred_proba_final = final_model.predict_proba(X_test_imputed)[:, 1] # Probability of the positive class\n",
        "\n",
        "print(\"\\nGenerating classification report and confusion matrix for the final model...\")\n",
        "\n",
        "# Generate Classification Report\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "\n",
        "print(\"\\nClassification Report:\")\n",
        "print(classification_report(y_test, y_pred_final))\n",
        "\n",
        "# Generate Confusion Matrix\n",
        "conf_matrix = confusion_matrix(y_test, y_pred_final)\n",
        "\n",
        "print(\"\\nConfusion Matrix:\")\n",
        "print(conf_matrix)\n",
        "\n",
        "# Visualize the Confusion Matrix\n",
        "plt.figure(figsize=(8, 6))\n",
        "sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues', cbar=False,\n",
        "            xticklabels=['Predicted Negative', 'Predicted Positive'],\n",
        "            yticklabels=['Actual Negative', 'Actual Positive'])\n",
        "plt.xlabel('Predicted Label')\n",
        "plt.ylabel('True Label')\n",
        "plt.title('Confusion Matrix for Final Model')\n",
        "plt.show()\n",
        "\n",
        "# Optional: Visualize the ROC curve\n",
        "from sklearn.metrics import roc_curve, roc_auc_score\n",
        "\n",
        "print(\"\\nGenerating ROC curve for the final model...\")\n",
        "fpr, tpr, thresholds = roc_curve(y_test, y_pred_proba_final)\n",
        "auc = roc_auc_score(y_test, y_pred_proba_final)\n",
        "\n",
        "plt.figure(figsize=(8, 6))\n",
        "plt.plot(fpr, tpr, color='blue', lw=2, label=f'ROC Curve (AUC = {auc:.4f})')\n",
        "plt.plot([0, 1], [0, 1], color='gray', linestyle='--')\n",
        "plt.xlim([0.0, 1.0])\n",
        "plt.ylim([0.0, 1.05])\n",
        "plt.xlabel('False Positive Rate')\n",
        "plt.ylabel('True Positive Rate')\n",
        "plt.title('Receiver Operating Characteristic (ROC) Curve')\n",
        "plt.legend(loc=\"lower right\")\n",
        "plt.grid(True)\n",
        "plt.show()\n",
        "\n",
        "print(\"\\nâœ… Final model predictions implemented and visualized.\")\n"
      ],
      "id": "7eae6122",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Tying up some missing aspects**\n",
        "\n",
        "> F1 assessment, Outliers logging and feature importances\n",
        "\n"
      ],
      "metadata": {
        "id": "7dKuG8anPZPO"
      },
      "id": "7dKuG8anPZPO"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "abbe00a8"
      },
      "source": [
        "## Compare Model Performance: AUC vs. F1 Optimization\n",
        "\n",
        "Based on the previous evaluations, we will compare the performance of the best model tuned for AUC (Gradient Boosting with ADASYN) with other models or techniques that showed promising F1-scores, Precision, or Recall, to determine which approach is most effective for achieving higher prediction of failure with better precision and recall."
      ],
      "id": "abbe00a8"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c30a4c34"
      },
      "source": [
        "# Convert the evaluation_metrics dictionary into a pandas DataFrame for easier comparison\n",
        "evaluation_df_all = pd.DataFrame(evaluation_metrics).T\n",
        "\n",
        "print(\"Comparing Model Performance (Focus on Precision, Recall, and F1-score):\")\n",
        "# Display relevant columns and sort by a metric like F1-score or Recall to highlight models that perform well on the minority class\n",
        "display(evaluation_df_all[[ 'Precision', 'Recall', 'F1-score', 'AUC']].sort_values(by='F1-score', ascending=False))\n",
        "\n",
        "print(\"\\nAnalysis:\")\n",
        "print(\"- The table above shows the Precision, Recall, F1-score, and AUC for all evaluated models and techniques, sorted by F1-score.\")\n",
        "print(\"- We are specifically looking for approaches that yield a better balance between Precision and Recall for the minority class (failures).\")\n",
        "\n",
        "# Discuss the trade-offs observed (e.g., higher Recall often comes with lower Precision).\n",
        "best_f1_model_name = evaluation_df_all['F1-score'].idxmax()\n",
        "best_f1_metrics = evaluation_df_all.loc[best_f1_model_name]\n",
        "\n",
        "print(f\"\\nModel with the highest F1-score: {best_f1_model_name}\")\n",
        "print(f\"Metrics for {best_f1_model_name}:\")\n",
        "print(f\"  Precision: {best_f1_metrics['Precision']:.4f}\")\n",
        "print(f\"  Recall: {best_f1_metrics['Recall']:.4f}\")\n",
        "print(f\"  F1-score: {best_f1_metrics['F1-score']:.4f}\")\n",
        "print(f\"  AUC: {best_f1_metrics['AUC']:.4f}\")\n",
        "\n",
        "\n",
        "# Compare this to the model tuned for AUC (Tuned_GradientBoosting_ADASYN)\n",
        "adasyn_model_name = 'Tuned_GradientBoosting_ADASYN'\n",
        "if adasyn_model_name in evaluation_df_all.index:\n",
        "    adasyn_metrics = evaluation_df_all.loc[adasyn_model_name]\n",
        "    print(f\"\\nMetrics for {adasyn_model_name} (Highest AUC):\")\n",
        "    print(f\"  Precision: {adasyn_metrics['Precision']:.4f}\")\n",
        "    print(f\"  Recall: {adasyn_metrics['Recall']:.4f}\")\n",
        "    print(f\"  F1-score: {adasyn_metrics['F1-score']:.4f}\")\n",
        "    print(f\"  AUC: {adasyn_metrics['AUC']:.4f}\")\n",
        "\n",
        "    print(\"\\nComparison Insights:\")\n",
        "\n",
        "    # Add insights based on the comparison:\n",
        "    if best_f1_model_name != adasyn_model_name:\n",
        "         print(f\"- The model with the highest F1-score ({best_f1_model_name}) does not have the highest AUC.\")\n",
        "         print(f\"- Comparing Precision and Recall:\")\n",
        "         print(f\"  - {best_f1_model_name}: Precision = {best_f1_metrics['Precision']:.4f}, Recall = {best_f1_metrics['Recall']:.4f}\")\n",
        "         print(f\"  - {adasyn_model_name}: Precision = {adasyn_metrics['Precision']:.4f}, Recall = {adasyn_metrics['Recall']:.4f}\")\n",
        "         # Add further analysis here based on the specific values and business problem\n",
        "    else:\n",
        "         print(f\"- The model with the highest F1-score is also the one with the highest AUC ({best_f1_model_name}).\")\n",
        "         print(\"  This indicates that ADASYN resampling combined with Gradient Boosting tuning was effective.\")\n",
        "\n",
        "# Based on the analysis, reiterate which model/technique seems most promising for the objective\n",
        "print(\"\\nConclusion for predicting failures with higher Precision and Recall:\")\n",
        "# State which model appears to offer the best trade-off or performance for predicting failures, considering the business goal.\n",
        "print(f\"Based on the metrics, the '{best_f1_model_name}' model currently shows the highest F1-score, indicating the best balance between Precision and Recall for predicting failures. However, the overall F1-score is still low, highlighting the challenge of this imbalanced dataset.\")\n",
        "print(\"Further analysis of the trade-offs between Precision and Recall based on the specific costs of false positives and false negatives in the manufacturing process would be needed for a definitive 'best' model for deployment.\")"
      ],
      "id": "c30a4c34",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dc245f82"
      },
      "source": [
        "## Implement and Visualize Highest F1-score Model Predictions\n"
      ],
      "id": "dc245f82"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d57fe96a"
      },
      "source": [
        "print(\"Implementing and visualizing predictions of the highest F1-score model ('Tuned_GradientBoosting') on the test set...\")\n",
        "\n",
        "# Retrieve the 'Tuned_GradientBoosting' model object\n",
        "# This model was trained and evaluated in earlier steps (e.g., MyBy5e_R1OKI and 9598defb)\n",
        "# Assuming the 'best_tuned_model' variable holds the Tuned_GradientBoosting model (tuned for AUC originally, but used as the base for comparison)\n",
        "# The comparison in c30a4c34 identified 'Tuned_GradientBoosting' as having the highest F1. This corresponds to the model stored in 'best_tuned_model'.\n",
        "try:\n",
        "    best_tuned_model\n",
        "except NameError:\n",
        "    print(\"Variable 'best_tuned_model' not found. Please ensure hyperparameter tuning (cell H6z0IDGD4to5) was run successfully.\")\n",
        "    # Exit or handle the error appropriately if the variable is not available\n",
        "    # For now, assume best_tuned_model is available\n",
        "\n",
        "\n",
        "# Use the 'best_tuned_model' which corresponds to 'Tuned_GradientBoosting'\n",
        "highest_f1_model = best_tuned_model\n",
        "\n",
        "# Make predictions on the original imputed test set\n",
        "y_pred_highest_f1 = highest_f1_model.predict(X_test_imputed)\n",
        "y_pred_proba_highest_f1 = highest_f1_model.predict_proba(X_test_imputed)[:, 1] # Probability of the positive class\n",
        "\n",
        "print(\"\\nGenerating classification report and confusion matrix for the highest F1-score model...\")\n",
        "\n",
        "# Generate Classification Report\n",
        "from sklearn.metrics import classification_report, confusion_matrix, roc_auc_score\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "print(\"\\nClassification Report:\")\n",
        "print(classification_report(y_test, y_pred_highest_f1))\n",
        "\n",
        "# Generate Confusion Matrix\n",
        "conf_matrix_highest_f1 = confusion_matrix(y_test, y_pred_highest_f1)\n",
        "\n",
        "print(\"\\nConfusion Matrix:\")\n",
        "print(conf_matrix_highest_f1)\n",
        "\n",
        "# Visualize the Confusion Matrix\n",
        "plt.figure(figsize=(8, 6))\n",
        "sns.heatmap(conf_matrix_highest_f1, annot=True, fmt='d', cmap='Blues', cbar=False,\n",
        "            xticklabels=['Predicted Negative', 'Predicted Positive'],\n",
        "            yticklabels=['Actual Negative', 'Actual Positive'])\n",
        "plt.xlabel('Predicted Label')\n",
        "plt.ylabel('True Label')\n",
        "plt.title('Confusion Matrix for Highest F1-score Model (Tuned Gradient Boosting)')\n",
        "plt.show()\n",
        "\n",
        "# Visualize the ROC curve\n",
        "from sklearn.metrics import roc_curve\n",
        "\n",
        "print(\"\\nGenerating ROC curve for the highest F1-score model...\")\n",
        "fpr_highest_f1, tpr_highest_f1, thresholds_highest_f1 = roc_curve(y_test, y_pred_proba_highest_f1)\n",
        "auc_highest_f1 = roc_auc_score(y_test, y_pred_proba_highest_f1)\n",
        "\n",
        "plt.figure(figsize=(8, 6))\n",
        "plt.plot(fpr_highest_f1, tpr_highest_f1, color='blue', lw=2, label=f'ROC Curve (AUC = {auc_highest_f1:.4f})')\n",
        "plt.plot([0, 1], [0, 1], color='gray', linestyle='--')\n",
        "plt.xlim([0.0, 1.0])\n",
        "plt.ylim([0.0, 1.05])\n",
        "plt.xlabel('False Positive Rate')\n",
        "plt.ylabel('True Positive Rate')\n",
        "plt.title('Receiver Operating Characteristic (ROC) Curve for Highest F1-score Model')\n",
        "plt.legend(loc=\"lower right\")\n",
        "plt.grid(True)\n",
        "plt.show()\n",
        "\n",
        "print(\"\\nâœ… Highest F1-score model predictions implemented and visualized.\")\n",
        "\n",
        "# Print the technical report for the highest F1 model\n",
        "# try:\n",
        "#     print(report_f1)\n",
        "# except NameError:\n",
        "#     print(\"\\nTechnical report for Highest F1 model not available. Please run the report generation cell (0847f5d2) first.\")"
      ],
      "id": "d57fe96a",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a7462fc7"
      },
      "source": [
        "## Outlier logging\n",
        "\n",
        "Outliers have been detected and logged in the data using the Interquartile Range (IQR) method (as shown in the code below). However, they were **not handled** (e.g., removed, transformed, or imputed) in this analysis.\n",
        "\n",
        "This decision was made because:\n",
        "*   Handling outliers effectively, especially in a high-dimensional and complex manufacturing dataset, requires careful consideration and **domain expertise** to understand their potential causes and impact on the manufacturing process.\n",
        "*   Applying standard outlier handling methods without domain knowledge could inadvertently **change the nature of the data** and potentially remove valuable information or introduce bias.\n",
        "*   Addressing outlier handling was considered **out of the scope of the current time and effort availability** for this project iteration.\n",
        "\n",
        "The detected outlier indices have been logged to files for **review by a domain expert**. A domain expert could provide insights into whether these outliers represent true anomalies, measurement errors, or critical process variations, and guide the appropriate handling strategy.\n",
        "\n",
        "**Potential Outlier Handling Methods (for Domain Expert Review):**\n",
        "\n",
        "Under the supervision and guidance of a domain expert, the following methods could be considered for handling outliers:\n",
        "\n",
        "*   **Removal:** Removing rows or columns containing outliers. This is often the simplest but can lead to loss of valuable data, especially in imbalanced datasets.\n",
        "*   **Capping/Winsorizing:** Limiting extreme values to a certain threshold (e.g., replacing values outside 1.5 or 3 times the IQR with the boundary value). This reduces the impact of outliers without removing data.\n",
        "*   **Transformation:** Applying mathematical transformations (e.g., logarithmic, square root) to features to reduce the skewness caused by outliers.\n",
        "*   **Imputation:** Treating outliers as missing values and using imputation techniques (potentially robust ones like median or trimmed mean imputation, or model-based imputation) to replace them.\n",
        "*   **Model-Based Methods:** Using models that are less sensitive to outliers (e.g., tree-based models like Random Forest or Gradient Boosting are generally more robust than linear models or SVMs to outliers).\n",
        "*   **Separate Modeling:** In some cases, outliers might represent a distinct class or phenomenon that could be modeled separately.\n",
        "\n",
        "The appropriate method would depend heavily on the nature of the outliers and the goals of the predictive model, as determined by a manufacturing domain expert."
      ],
      "id": "a7462fc7"
    },
    {
      "cell_type": "code",
      "source": [
        "# Code block for Outlier Detection and Logging\n",
        "\n",
        "print(\"Performing basic outlier detection and logging...\")\n",
        "\n",
        "# Re-apply the preprocessing steps up to imputation to get the data state before modeling for outlier detection\n",
        "# (assuming we want to detect outliers in the imputed data before feeding it to models)\n",
        "\n",
        "# Note: A more thorough outlier analysis might involve detecting outliers BEFORE imputation as well,\n",
        "# or using domain-specific methods. This is a basic example.\n",
        "\n",
        "# We will use the X_train_imputed and X_test_imputed data for outlier detection.\n",
        "# A simple method is using the Interquartile Range (IQR).\n",
        "\n",
        "def detect_outliers_iqr(df):\n",
        "    outlier_indices = set()\n",
        "    for col in df.columns:\n",
        "        if np.issubdtype(df[col].dtype, np.number): # Only check numeric columns\n",
        "            Q1 = df[col].quantile(0.25)\n",
        "            Q3 = df[col].quantile(0.75)\n",
        "            IQR = Q3 - Q1\n",
        "            lower_bound = Q1 - 1.5 * IQR\n",
        "            upper_bound = Q3 + 1.5 * IQR\n",
        "            # Find indices where values are outside the bounds\n",
        "            col_outlier_indices = df[(df[col] < lower_bound) | (df[col] > upper_bound)].index\n",
        "            outlier_indices.update(col_outlier_indices)\n",
        "    return list(outlier_indices)\n",
        "\n",
        "# Detect outliers in the imputed training data\n",
        "train_outlier_indices = detect_outliers_iqr(pd.DataFrame(X_train_imputed, columns=[f'feature_{i}' for i in range(X_train_imputed.shape[1])])) # Convert back to DataFrame for column handling\n",
        "\n",
        "# Detect outliers in the imputed test data\n",
        "test_outlier_indices = detect_outliers_iqr(pd.DataFrame(X_test_imputed, columns=[f'feature_{i}' for i in range(X_test_imputed.shape[1])])) # Convert back to DataFrame\n",
        "\n",
        "print(f\"Found {len(train_outlier_indices)} outliers in the imputed training data (based on IQR).\")\n",
        "print(f\"Found {len(test_outlier_indices)} outliers in the imputed test data (based on IQR).\")\n",
        "\n",
        "# Log the indices of detected outliers to files\n",
        "os.makedirs(\"outlier_logs\", exist_ok=True)\n",
        "\n",
        "train_outlier_log_path = \"outlier_logs/train_outlier_indices_iqr.txt\"\n",
        "with open(train_outlier_log_path, \"w\") as f:\n",
        "    for idx in train_outlier_indices:\n",
        "        f.write(str(idx) + \"\\n\")\n",
        "print(f\"Logged training outlier indices to {train_outlier_log_path}\")\n",
        "\n",
        "test_outlier_log_path = \"outlier_logs/test_outlier_indices_iqr.txt\"\n",
        "with open(test_outlier_log_path, \"w\") as f:\n",
        "    for idx in test_outlier_indices:\n",
        "        f.write(str(idx) + \"\\n\")\n",
        "print(f\"Logged test outlier indices to {test_outlier_log_path}\")\n",
        "\n",
        "print(\"\\nNote: Outliers were detected and logged but not removed or transformed in this analysis, as per the limitations identified.\")"
      ],
      "metadata": {
        "id": "0S5rCRa2cz6B"
      },
      "id": "0S5rCRa2cz6B",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "31962278"
      },
      "source": [
        "## SHAP Feature Importance Analysis\n",
        "\n",
        "Using SHAP (SHapley Additive exPlanations) to understand feature importance and how features influence model predictions."
      ],
      "id": "31962278"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f8e29b8b"
      },
      "source": [
        "print(\"Performing SHAP feature importance analysis...\")\n",
        "\n",
        "# Use the best_tuned_model (Gradient Boosting tuned for AUC)\n",
        "try:\n",
        "    best_tuned_model\n",
        "except NameError:\n",
        "    print(\"Variable 'best_tuned_model' not found. Please ensure hyperparameter tuning (cell H6z0IDGD4to5) was run successfully.\")\n",
        "    # Exit or handle the error appropriately if the variable is not available\n",
        "    # For now, assume best_tuned_model is available\n",
        "\n",
        "# Use a subset of the training data for SHAP calculations to reduce computation time\n",
        "# Calculating SHAP values for the entire dataset can be very slow\n",
        "# Let's use a sample of the imputed training data\n",
        "# Corrected: Create DataFrame with generic column names to match the imputed data shape\n",
        "shap_data_sample = pd.DataFrame(X_train_imputed, columns=[f'Feature {i}' for i in range(X_train_imputed.shape[1])]).sample(n=1000, random_state=42) # Use generic column names\n",
        "\n",
        "# Create a SHAP TreeExplainer for tree-based models\n",
        "explainer = shap.TreeExplainer(best_tuned_model)\n",
        "\n",
        "# Calculate SHAP values for the sample data\n",
        "print(\"Calculating SHAP values (this may take some time)...\")\n",
        "shap_values = explainer.shap_values(shap_data_sample)\n",
        "print(\"SHAP value calculation complete.\")\n",
        "\n",
        "# Visualize the SHAP summary plot (global feature importance)\n",
        "print(\"\\nGenerating SHAP summary plot...\")\n",
        "shap.summary_plot(shap_values, shap_data_sample, plot_type=\"bar\", show=False)\n",
        "plt.title(\"SHAP Feature Importance (Mean Absolute SHAP Value)\")\n",
        "plt.show()\n",
        "\n",
        "# Visualize the SHAP summary plot (impact and direction)\n",
        "print(\"\\nGenerating SHAP summary plot (impact and direction)...\")\n",
        "shap.summary_plot(shap_values, shap_data_sample, show=False)\n",
        "plt.show()\n",
        "\n",
        "\n",
        "print(\"\\nâœ… SHAP feature importance analysis complete.\")\n",
        "\n",
        "# Note: For a deeper dive, you could analyze individual instance SHAP values (e.g., shap.initplot)\n",
        "# or interaction effects (e.g., shap.dependence_plot)."
      ],
      "id": "f8e29b8b",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ae9c3ce1"
      },
      "source": [
        "## Limitations and Future Work\n",
        "\n",
        "This section identifies the key limitations of the current model and approach, along with potential areas for future improvement and further research, taking into account the constraints faced during this project.\n",
        "\n",
        "### Limitations of the Current Approach:\n",
        "\n",
        "1.  **Data Usage due to Compute Resources:** Due to limitations in compute resources and memory, the analysis was performed on a sampled subset of the complete dataset. This means the model was trained on a smaller portion of the available data, which might impact its ability to capture all patterns present in the full dataset and generalize to all manufacturing variations. Processing the data in chunks and creating a smaller dataset was a necessary step to make the analysis feasible within the available environment.\n",
        "2.  **Simplified Imputation Strategy:** Simple median imputation was used for missing values in both numeric and date features. More sophisticated imputation techniques like KNNImputer or IterativeImputer were not used initially due to complexity, time, and compute constraints. These methods can potentially provide more accurate imputations by considering the relationships between features.\n",
        "3.  **Imputation Order and Potential (though minimal) Data Leakage:** While imputation was primarily done before the train-test split for simplicity and manageability in a high-dimensional dataset, there is a theoretical potential for minimal data leakage.\n",
        "    *   **Justification for Minimal Leakage:** For the initial column-wise imputation steps (100% missing, low variance, constant values), these were based on properties inherent to the columns themselves, independent of specific row values. For the median imputation *before* the split, while using the overall median might introduce a slight leak (as the test set median is implicitly used), the impact is expected to be minimal given the large dataset size and the nature of median imputation compared to methods that might use more complex patterns from the entire dataset. Deferring all imputation until after the split, especially for such a high-dimensional and complex dataset with various data types, was deemed beyond the scope of current time and effort availability, particularly when considering more advanced imputation methods.\n",
        "4.  **Handling of Advanced Preprocessing and Derived Features:** While some derived date features were created, more extensive feature engineering, especially involving interactions between features or creating features based on domain knowledge across the merged dataset, was limited by time and complexity. Implementing imputation (particularly KNN) after merging the large number of derived and original columns was also beyond the scope of the current effort.\n",
        "5.  **Feature Interactions:** Due to the large number of features, comprehensive checking and engineering of interaction terms among features was not performed. Potential synergistic effects between different sensor signals that might be predictive of failure may have been missed.\n",
        "6.  **Outlier Detection and Handling:** Outlier detection and handling were not explicitly performed in this analysis. Outliers in the data could potentially influence the model training and performance, especially for models sensitive to extreme values. A thorough outlier analysis often requires domain expertise to determine appropriate handling strategies (e.g., capping, transformation, removal).\n",
        "7.  **Randomized Search vs. Grid Search:** Hyperparameter tuning was performed using `RandomizedSearchCV` rather than `GridSearchCV`. While `RandomizedSearchCV` is more computationally efficient and often finds good hyperparameters, `GridSearchCV` performs an exhaustive search over the defined parameter grid, which could potentially find a better combination of hyperparameters if compute resources were not a constraint.\n",
        "8.  **Model Regularization:** The Gradient Boosting model has built-in regularization parameters (`learning_rate`, `max_depth`, `min_samples_split`, `min_samples_leaf`, `subsample`, etc.). Tuning these parameters (as done in the hyperparameter tuning step) helps to control model complexity and reduce overfitting.\n",
        "    *   **Addressing Overfitting Resolution:** The learning curve for the model showed signs of slight overfitting (training score higher than cross-validation score). The regularization parameters tuned during `RandomizedSearchCV` (reducing `max_depth`, increasing `min_samples_split`/`min_samples_leaf`, using a smaller `learning_rate` with sufficient `n_estimators`) are intended to mitigate this. Increasing `n_estimators` alone can worsen overfitting without other controls. More data is generally beneficial but, in this case, we used a sampled chunk of the available data. While we couldn't use *all* data at once due to compute limits, the approach of sampling allows for potentially training on *different* chunks of data in the future if needed. Exploring more aggressive regularization techniques or simpler model architectures could also help if overfitting persists.\n",
        "9.  **Evaluation Metric Focus:** The initial hyperparameter tuning and model selection primarily focused on maximizing AUC, which is a good overall metric for imbalanced datasets. However, for predicting product failures, Precision (minimizing false positives) and Recall (minimizinf false negatives) are often critical business metrics. While AUC was prioritized, the analysis did look at other metrics, and a comparison specifically focused on optimizing for F1-score (balancing Precision and Recall) was suggested as a future step.\n",
        "    *   **Achieving Higher Precision and Recall:** The current model (Gradient Boosting with ADASYN, tuned for AUC) achieved a Precision of 0.1111 and a Recall of 0.0667 on the test set. Ideally, for a manufacturing failure prediction system, we would aim for significantly higher values for both Precision and Recall to minimize both false positives and false negatives. The F1-score was 0.0833.\n",
        "    *   **Outcome of F1 Optimization Attempt:** We compared models based on F1-score (in cell `c30a4c34`) and found the highest F1-score achieved by any model evaluated was **0.1111** (by the Tuned Gradient Boosting model, without explicit resampling before tuning). Threshold adjustment on the AUC-tuned model (in cell `d81389f6`) could slightly improve its F1-score to **0.1053**, but did not surpass the highest observed F1. This indicates that optimizing solely for F1 or adjusting the threshold on the current model did not lead to a significant breakthrough in balancing Precision and Recall. Achieving substantially higher values for both metrics likely requires addressing the fundamental limitations outlined above.\n",
        "\n",
        "### Future Work and Potential Improvements:\n",
        "\n",
        "1.  **Explore Full Dataset or Larger Chunks:** If computational resources become available, train the model on the full dataset or significantly larger sampled chunks to potentially improve generalization.\n",
        "2.  **Advanced Imputation Techniques:** Investigate and implement more sophisticated imputation methods (e.g., KNNImputer, IterativeImputer) after the train-test split to potentially handle missing values more effectively and prevent any form of data leakage.\n",
        "3.  **Comprehensive Outlier Analysis and Handling:** Perform a dedicated outlier detection and handling step, potentially involving domain experts to understand the nature of outliers and choose appropriate mitigation strategies. Log detected outliers for review.\n",
        "4.  **Grid Search for Hyperparameter Tuning:** Conduct a more exhaustive `GridSearchCV` for hyperparameter tuning if compute resources allow, to potentially find a better set of hyperparameters.\n",
        "5.  **Cost-Sensitive Learning:** Implement cost-sensitive learning approaches, where the model is trained to incur a higher penalty for misclassifying the minority class (false negatives) or the majority class (false positives), based on the specific business costs associated with each type of error.\n",
        "6.  **Explore Different Resampling Strategies or Ratios:** Experiment with other advanced resampling techniques (e.g., Borderline-SMOTE variants, SMOTE-NC for mixed data types if applicable) or different resampling ratios to find the optimal balance for the dataset.\n",
        "7.  **Investigate Other Imbalanced Ensemble Methods:** Further tune and evaluate imbalanced ensemble methods (like EasyEnsembleClassifier) which showed promising Recall or AUC in initial runs, to see if their Precision can be improved.\n",
        "8.  **Deep Feature Engineering:** Conduct more in-depth feature engineering based on domain expertise to create features that are more predictive of product failures.\n",
        "9.  **Explore Alternative Model Architectures:** Evaluate other model architectures that are known to perform well on imbalanced data or high-dimensional sparse data (e.g., LightGBM with appropriate parameters for imbalance, CatBoost, or even deep learning approaches if justified by data volume and complexity).\n",
        "10. **Threshold Adjustment:** Analyze the precision-recall curve and potentially adjust the classification threshold of the final model to favor higher Recall or higher Precision, depending on the specific business requirements (e.g., is it more critical to catch all failures, even with false alarms, or to minimize false alarms, even if some failures are missed?).\n",
        "11. **Feature Importance and Interpretability Deep Dive:** Conduct a detailed analysis of feature importance (using methods like permutation importance) to gain deeper insights into which specific sensor signals are most indicative of failure risk, providing actionable insights for manufacturing engineers."
      ],
      "id": "ae9c3ce1"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7c5f8d9c"
      },
      "source": [
        "## Addon: Explore Classification Threshold Adjustment to verify if we can achieve any more precision recall simultaneously under given conditions\n",
        "\n",
        "Given the low Precision and Recall of the current models, let's explore how adjusting the classification threshold of the best tuned model (optimized for AUC) impacts these metrics. This can help us find a threshold that offers a better trade-off between Precision and Recall based on the specific business needs."
      ],
      "id": "7c5f8d9c"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d81389f6"
      },
      "source": [
        "print(\"Exploring classification threshold adjustment...\")\n",
        "\n",
        "# Use the predicted probabilities from the final model (tuned for AUC + ADASYN)\n",
        "# Assuming y_pred_proba_final from cell 7eae6122 is available\n",
        "try:\n",
        "    y_pred_proba_final\n",
        "except NameError:\n",
        "    print(\"Predicted probabilities (y_pred_proba_final) not found. Please run cell 7eae6122 first.\")\n",
        "    # Exit or handle the error appropriately if the variable is not available\n",
        "    # For now, let's assume it's available from the previous execution\n",
        "\n",
        "# Calculate Precision-Recall curve\n",
        "precision, recall, thresholds = precision_recall_curve(y_test, y_pred_proba_final)\n",
        "\n",
        "# Calculate AUC for the Precision-Recall curve\n",
        "pr_auc = auc_score(recall, precision)\n",
        "\n",
        "print(f\"Precision-Recall AUC: {pr_auc:.4f}\")\n",
        "\n",
        "# Plot the Precision-Recall curve\n",
        "plt.figure(figsize=(8, 6))\n",
        "plt.plot(recall, precision, marker='.', label=f'Precision-Recall Curve (AUC = {pr_auc:.4f})')\n",
        "plt.xlabel('Recall')\n",
        "plt.ylabel('Precision')\n",
        "plt.title('Precision-Recall Curve for Final Model')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "\n",
        "print(\"\\nAnalyzing metrics at different thresholds...\")\n",
        "\n",
        "# Create a DataFrame to view Precision, Recall, and F1-score at different thresholds\n",
        "# Align the arrays: thresholds has one less element than precision and recall\n",
        "# Use thresholds as the index and slice precision and recall to match its length\n",
        "threshold_metrics = pd.DataFrame({\n",
        "    'Threshold': thresholds,\n",
        "    'Precision': precision[:-1], # Slice to exclude the last element\n",
        "    'Recall': recall[:-1]       # Slice to exclude the last element\n",
        "})\n",
        "\n",
        "# Calculate F1-score for each threshold\n",
        "# Avoid division by zero if Precision and Recall are both 0\n",
        "threshold_metrics['F1-score'] = 2 * (threshold_metrics['Precision'] * threshold_metrics['Recall']) / (threshold_metrics['Precision'] + threshold_metrics['Recall'])\n",
        "threshold_metrics['F1-score'] = threshold_metrics['F1-score'].fillna(0) # Fill NaN (where P+R=0) with 0\n",
        "\n",
        "# Display metrics at a few key thresholds or sorted by F1-score\n",
        "print(\"\\nMetrics at various classification thresholds (sorted by F1-score):\")\n",
        "display(threshold_metrics.sort_values(by='F1-score', ascending=False).head(10)) # Display top 10 by F1-score\n",
        "\n",
        "print(\"\\nMetrics at various classification thresholds (sorted by Threshold):\")\n",
        "display(threshold_metrics.sort_values(by='Threshold', ascending=True).head(10)) # Display first 10 thresholds\n",
        "\n",
        "# Find the threshold that maximizes F1-score\n",
        "best_f1_threshold_row = threshold_metrics.loc[threshold_metrics['F1-score'].idxmax()]\n",
        "best_f1_threshold = best_f1_threshold_row['Threshold']\n",
        "best_f1_at_threshold = best_f1_threshold_row['F1-score']\n",
        "precision_at_best_f1_threshold = best_f1_threshold_row['Precision']\n",
        "recall_at_best_f1_threshold = best_f1_threshold_row['Recall']\n",
        "\n",
        "\n",
        "print(f\"\\nThreshold that maximizes F1-score: {best_f1_threshold:.4f}\")\n",
        "print(f\"  F1-score at this threshold: {best_f1_at_threshold:.4f}\")\n",
        "print(f\"  Precision at this threshold: {precision_at_best_f1_threshold:.4f}\")\n",
        "print(f\"  Recall at this threshold: {recall_at_best_f1_threshold:.4f}\")\n",
        "\n",
        "# Add a marker to the plot at the point maximizing F1-score\n",
        "plt.plot(recall_at_best_f1_threshold, precision_at_best_f1_threshold, 'ro', markersize=8, label=f'Max F1 (F1={best_f1_at_threshold:.4f})')\n",
        "plt.legend()\n",
        "\n",
        "\n",
        "# We may also find thresholds that give a specific level of Recall or Precision\n",
        "# Example: Find the threshold for a Recall of at least 0.5 (if achievable)\n",
        "desired_recall = 0.5\n",
        "thresholds_for_desired_recall = threshold_metrics[threshold_metrics['Recall'] >= desired_recall]\n",
        "\n",
        "if not thresholds_for_desired_recall.empty:\n",
        "    # Find the threshold among these that maximizes Precision\n",
        "    best_threshold_for_recall = thresholds_for_desired_recall.sort_values(by='Precision', ascending=False).iloc[0]\n",
        "    print(f\"\\nThreshold to achieve Recall >= {desired_recall}: {best_threshold_for_recall['Threshold']:.4f}\")\n",
        "    print(f\"  Precision at this threshold: {best_threshold_for_recall['Precision']:.4f}\")\n",
        "    print(f\"  Recall at this threshold: {best_threshold_for_recall['Recall']:.4f}\")\n",
        "    print(f\"  F1-score at this threshold: {best_threshold_for_recall['F1-score']:.4f}\")\n",
        "else:\n",
        "    print(f\"\\nCould not achieve a Recall of {desired_recall} with the current model by adjusting the threshold.\")\n",
        "\n",
        "# Create a directory for saving images if it doesn't exist\n",
        "if not os.path.exists('images'):\n",
        "    os.makedirs('images')\n",
        "\n",
        "# Save the plot to a file\n",
        "plt.savefig('images/precision_recall_curve.png', bbox_inches='tight')\n",
        "\n",
        "plt.show() # Ensure plot is shown after adding the marker\n",
        "\n",
        "\n",
        "print(\"\\nâœ… Classification threshold analysis complete.\")"
      ],
      "id": "d81389f6",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c494aece"
      },
      "source": [
        "## Deduction from classification threshold checks\n",
        "\n",
        "Based on the comprehensive analysis of various models, hyperparameter tuning (optimizing for AUC), exploration of resampling techniques, and analysis of classification threshold adjustments, we have made significant progress in building a predictive model for product failures in a high-dimensional manufacturing process.\n",
        "\n",
        "We explored different approaches to address the severe class imbalance, including oversampling (SMOTE, ADASYN, BorderlineSMOTE), undersampling (RandomUnderSampler), combined techniques (SMOTE-ENN, SMOTE-Tomek), and imbalanced ensemble methods. Hyperparameter tuning was performed on the Gradient Boosting Classifier, initially optimizing for AUC.\n",
        "\n",
        "The model selected for detailed analysis and final implementation was the **Gradient Boosting Classifier with ADASYN Resampling**, primarily because it achieved the highest AUC (0.6522) among the evaluated models. However, the evaluation revealed that achieving high Precision and Recall for the minority class (failures) remains a significant challenge with this model and dataset under the current constraints.\n",
        "\n",
        "The highest F1-score achieved by any model evaluated was **0.1111** (by the Tuned Gradient Boosting model, without explicit resampling before tuning). While threshold adjustment on the AUC-tuned model could slightly improve its F1-score, it did not surpass this value. The analysis clearly showed the inherent trade-off between Precision and Recall on this imbalanced dataset, as visualized by the Precision-Recall curve. Achieving high Recall (e.g., 0.80) resulted in extremely low Precision (e.g., 0.0102), and vice versa.\n",
        "\n",
        "**Conclusion on Achieving Aim:**\n",
        "\n",
        "While the project successfully implemented a pipeline for preprocessing high-dimensional data and training classification models, and identified the Gradient Boosting Classifier with ADASYN Resampling as the best performer based on AUC, the goal of achieving a high prediction rate for failures with both high Precision and Recall was not fully met with the current methods and computational limitations. The low F1-score (maximum 0.1111) highlights that the model still struggles to balance identifying actual failures with minimizing false positives.\n",
        "\n",
        "**Conclusion on Further Improvement (Modeling Part):**\n",
        "\n",
        "Given the comprehensive exploration within the current constraints (sampled data, simplified imputation, etc.), it is likely that significant improvements in Precision and Recall are **not achievable solely by further tuning of these specific models or adjusting thresholds** without addressing the underlying limitations. While minor gains might be possible, a breakthrough in performance for the minority class would likely require tackling the challenges outlined in the \"Limitations and Future Work\" section.\n",
        "\n",
        "**Overall Summary:**\n",
        "\n",
        "The project successfully demonstrated a workflow for tackling this complex, imbalanced dataset. The best model achieved an AUC of 0.6522. However, the critical business metrics of Precision and Recall for predicting failures remain low (highest F1 of 0.1111).\n",
        "\n",
        "**Next Steps:**\n",
        "\n",
        "To significantly improve the prediction of product failures with higher Precision and Recall, the future work should focus on addressing the key limitations, particularly:\n",
        "\n",
        "*   Leveraging the full dataset (if compute permits).\n",
        "*   Implementing more sophisticated imputation and feature engineering techniques.\n",
        "*   Exploring cost-sensitive learning or more advanced ensemble methods designed for extreme imbalance.\n",
        "*   Potentially incorporating domain expertise for feature engineering and outlier handling.\n",
        "\n",
        "The analysis provides a solid foundation and a clear understanding of the challenges and the most promising avenues for future research to build a truly production-ready model for predicting manufacturing failures."
      ],
      "id": "c494aece"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "df98a441"
      },
      "source": [
        "# Technical Brief: Product Failure Prediction\n",
        "\n",
        "This report summarizes the machine learning project aimed at predicting product failures in a high-dimensional manufacturing process, detailing the problem, methodology, models, and evaluation.\n",
        "\n",
        "## 1. Problem Statement\n",
        "\n",
        "**Define the Problem Statement:**\n",
        "The goal of this project is to accurately predict product failures in a high-dimensional manufacturing process using presence-based signal extraction and interpretable machine learning techniques. The challenge lies in the severe class imbalance, where failures are rare events. By identifying failure-prone products early, manufacturers can implement proactive quality control measures, reducing costs and risks.\n",
        "\n",
        "## 2. Model Outcomes or Predictions\n",
        "\n",
        "**Type of Learning and Expected Output:**\n",
        "This project uses **supervised machine learning** for a **binary classification** task. The expected output of the selected models is a prediction of whether a product is likely to fail (Class 1) or not fail (Class 0). The models also output probability scores, which are used for evaluation metrics like AUC and for classification threshold adjustment.\n",
        "\n",
        "## 3. Data Acquisition\n",
        "\n",
        "**Data Source and Analysis:**\n",
        "The data used for this project is the **Bosch Production Line Performance dataset**, originally sourced from Kaggle.\n",
        "\n",
        "*Note: Due to computational constraints, the analysis in this notebook was performed on a sampled subset of the complete dataset. The original data consisted of numerous sensor readings (numeric and categorical) and timestamp data across different production lines.*\n",
        "\n",
        "*Initial EDA (performed prior to this notebook) would typically involve visualizations to assess the raw data's potential, such as examining the distribution of the target variable (Response), visualizing missing data patterns, and exploring initial correlations or distributions of key features. Such detailed visualizations on the full, high-dimensional raw dataset were not feasible within this notebook's environment after sampling.*\n",
        "\n",
        "**Initial Data Visualizations (Placeholders):**\n",
        "\n",
        "![Initial Data Acquisition Visuals Placeholder](path/to/initial_data_acquisition_visuals.png)\n",
        "\n",
        "![EDA Visuals Placeholder](path/to/eda_visuals.png)\n",
        "\n",
        "\n",
        "## 4. Data Preprocessing/Preparation\n",
        "\n",
        "**Data Cleaning and Preparation Techniques:**\n",
        "The data preprocessing involved several steps to handle missing values, inconsistencies, and prepare the data for modeling in a high-dimensional context:\n",
        "\n",
        "a.  **Missing Values and Inconsistencies:**\n",
        "    *   Columns with 100% missing values were dropped.\n",
        "    *   For numeric features, missing values were imputed using the **median** of each column.\n",
        "    *   For categorical features, missing values were imputed with a dedicated **'Missing'** label.\n",
        "    *   For date features, missing values in derived numerical features (days since min date) were imputed using the **median**.\n",
        "    *   Basic outlier detection using the Interquartile Range (IQR) was performed and outlier indices were logged for future domain expert review, but outliers were **not removed or transformed** in this analysis due to the need for domain expertise and time constraints.\n",
        "\n",
        "b.  **Data Splitting:**\n",
        "    *   The data was split into training (80%) and testing (20%) sets using **stratified sampling** based on the 'Response' variable to ensure that the rare failure instances were proportionally represented in both sets.\n",
        "\n",
        "c.  **Analysis and Encoding Steps:**\n",
        "    *   **Feature Selection:**\n",
        "        *   Low/zero variance numeric columns were dropped.\n",
        "        *   Constant value categorical/date columns were dropped.\n",
        "        *   High-cardinality categorical columns (>50 unique values) were dropped.\n",
        "        *   Highly correlated numeric features (>0.9 correlation) were dropped based on the training set correlation matrix.\n",
        "        *   Features with low correlation (<0.01) with the target were dropped based on the training set.\n",
        "    *   **Feature Scaling:** Numeric features were scaled using **StandardScaler**.\n",
        "    *   **Encoding:** Categorical features were encoded using **Mean Response Encoding** based on the training set target variable.\n",
        "    *   **Derived Features:** Numerical features (days since minimum date) were derived from date columns.\n",
        "\n",
        "*Note: Due to the high dimensionality and compute constraints, more advanced preprocessing techniques like KNN imputation after merging all features, or extensive feature engineering including interaction terms, were not fully explored within the scope of this project.*\n",
        "\n",
        "## 5. Modeling\n",
        "\n",
        "**Machine Learning Algorithms Selected:**\n",
        "A range of classification algorithms were considered and evaluated for this problem:\n",
        "\n",
        "*   **Baseline Models:** Logistic Regression, Random Forest, Gradient Boosting, Support Vector Machine (SVC), and K-Nearest Neighbors.\n",
        "*   **Tuned Model:** Gradient Boosting Classifier (hyperparameter tuned).\n",
        "*   **Models with Resampling:** Gradient Boosting Classifier trained on data resampled using SMOTE (Oversampling), RandomUnderSampler (Undersampling), ADASYN, BorderlineSMOTE, SMOTEENN, and SMOTETomek.\n",
        "*   **Imbalanced Ensemble Methods:** BalancedBaggingClassifier, EasyEnsembleClassifier, BalancedRandomForestClassifier.\n",
        "\n",
        "## 6. Model Evaluation\n",
        "\n",
        "**Evaluation Metrics and Optimal Model Determination:**\n",
        "Given the severe class imbalance, standard Accuracy is not a reliable evaluation metric. The primary metrics used were:\n",
        "\n",
        "*   **AUC (Area Under the ROC Curve):** Measures the model's ability to discriminate between positive and negative classes. Robust to class imbalance.\n",
        "*   **Precision:** Proportion of correctly predicted positive instances out of all instances predicted as positive. Important for minimizing false positives.\n",
        "*   **Recall:** Proportion of correctly predicted positive instances out of all actual positive instances. Important for minimizing false negatives.\n",
        "*   **F1-score:** Harmonic mean of Precision and Recall, balancing both metrics.\n",
        "\n",
        "**Optimal Model Determination:**\n",
        "Models were evaluated on the original, unseen test set. The initial selection and tuning focused on maximizing **AUC** as a primary indicator of overall model performance on imbalanced data.\n",
        "\n",
        "*   The **Gradient Boosting Classifier with ADASYN Resampling** achieved the highest AUC (0.6522).\n",
        "*   However, when focusing on balancing Precision and Recall, the **Tuned Gradient Boosting** model (without explicit resampling before tuning) achieved the highest F1-score (0.1111).\n",
        "\n",
        "The analysis of the Precision-Recall curve showed the inherent trade-off: increasing Recall significantly reduced Precision, and vice versa. Adjusting the classification threshold of the AUC-tuned model could slightly improve its F1-score (to 0.1053) but did not surpass the highest F1 observed.\n",
        "\n",
        "Due to the low F1-scores across all evaluated models under the current constraints, achieving a high prediction rate for failures with simultaneously high Precision and Recall was not fully realized.\n",
        "\n",
        "**Summary of Key Model Performances:**\n",
        "\n",
        "*   **Highest AUC Model:** Gradient Boosting with ADASYN Resampling\n",
        "    *   Accuracy: 0.9916\n",
        "    *   Precision: 0.1111\n",
        "    *   Recall: 0.0667\n",
        "    *   F1-score: 0.0833\n",
        "    *   AUC: 0.6522\n",
        "\n",
        "*   **Highest F1 Model:** Tuned Gradient Boosting (No explicit resampling before tuning)\n",
        "    *   Accuracy: 0.9939\n",
        "    *   Precision: 0.3333\n",
        "    *   Recall: 0.0667\n",
        "    *   F1-score: 0.1111\n",
        "    *   AUC: 0.6368\n",
        "\n",
        "**Visual Justification of Precision-Recall Trade-off (Placeholder):**\n",
        "\n",
        "[Embed Precision-Recall Curve Plot Here - from cell `d81389f6`]\n",
        "\n",
        "## Conclusion on Modeling:\n",
        "\n",
        "While the Gradient Boosting Classifier with ADASYN Resampling showed the best discriminative power (AUC), and the Tuned Gradient Boosting model achieved the highest F1-score, the overall performance in terms of accurately identifying the minority class with a good balance of Precision and Recall remains low. Significant improvements in these metrics would likely require addressing the limitations outlined in the notebook's \"Limitations and Future Work\" section, rather than further tuning or threshold adjustment alone with the current data and processing."
      ],
      "id": "df98a441"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f8a1109d"
      },
      "source": [
        "# Project Summary for Business Audience: Predicting Product Failures\n",
        "\n",
        "This report summarizes our project to predict product failures in manufacturing, focusing on the business impact and future potential.\n",
        "\n",
        "## 1. Problem Statement: Minimizing Costly Failures\n",
        "\n",
        "Our core problem is to proactively identify products likely to fail during or after production. Undetected failures are costly, leading to rework, warranty expenses, and damage to our brand. A successful prediction system would allow us to intervene early, improving quality and reducing waste. A key challenge is the rarity of failures â€“ predicting these few events among many good products is difficult.\n",
        "\n",
        "## 2. Model Outcomes: Identifying Risk\n",
        "\n",
        "Our system uses data analysis to predict whether a product is high-risk (likely to fail) or low-risk (likely to be good). It provides a risk score for each product. This is a supervised learning approach, meaning the system learned from past examples of products that either failed or did not fail.\n",
        "\n",
        "## 3. Data Used: Learning from Production Signals\n",
        "\n",
        "We used data from our production line, specifically the Bosch Production Line Performance dataset. This data contains valuable signals and measurements captured during manufacturing.\n",
        "\n",
        "*Note: Due to the immense size and complexity of the original dataset, our analysis was conducted on a representative sample. While a full analysis would ideally use all available data, this approach allowed us to build and evaluate initial models within practical limits.*\n",
        "\n",
        "**Visualizing Data Potential:** (To be filled with business-friendly visuals from initial data exploration, if available. E.g., graphs showing the distribution of good vs. bad products, patterns in key measurements related to failures.)\n",
        "\n",
        "![Initial Data Acquisition Visuals Placeholder](path/to/initial_data_acquisition_visuals.png)\n",
        "\n",
        "![EDA Visuals Placeholder](path/to/eda_visuals.png)\n",
        "\n",
        "\n",
        "## 4. Data Preparation: Getting Data Ready for Prediction\n",
        "\n",
        "We prepared the raw production data for analysis:\n",
        "\n",
        "*   **Cleaning Missing Information:** We addressed gaps in the data using automated methods to ensure the models received complete information.\n",
        "*   **Structuring Data:** We organized the data and created new measurements (e.g., time-based signals) to help the models find patterns.\n",
        "*   **Splitting for Testing:** We set aside a portion of the data as a \"blind test\" to ensure our models work well on products they haven't seen before, simulating real-world performance.\n",
        "\n",
        "*Approach Note: Some advanced data preparation techniques that could further improve results were not fully explored due to project constraints.*\n",
        "\n",
        "## 5. Predictive Models Explored\n",
        "\n",
        "We evaluated several predictive modeling techniques suitable for identifying risk based on complex data. We focused on methods capable of handling the challenge of rare failures, including standard models, tuned versions, and specialized techniques for imbalanced data.\n",
        "\n",
        "## 6. Model Performance: Strengths and Areas for Improvement\n",
        "\n",
        "We measured our models' performance using metrics relevant to identifying failures:\n",
        "\n",
        "*   **Overall Risk Ranking (AUC):** How well the model ranks high-risk products above low-risk ones. Our best model achieved an AUC of **0.6522**, showing a better-than-random ability to differentiate risk, but indicating room for improvement in its overall discriminative power.\n",
        "*   **Minimizing False Alarms (Precision):** When the model flags a product as high-risk, how often is it genuinely faulty? Our analysis showed the highest Precision achieved was **0.3333** (meaning 1 in 3 flagged products were actual failures) with one model, but this often came at the cost of missing many actual failures. With our primary selected model (optimized for overall risk ranking), Precision was **0.1111**.\n",
        "*   **Catching Actual Failures (Recall):** Out of all the products that actually failed, how many did our model successfully flag as high-risk? With our primary selected model, Recall was **0.0667**. To catch more failures, we could increase Recall (e.g., to **0.80**), but this drastically increases false alarms, driving Precision down to **0.0102**.\n",
        "*   **Balancing Alarms and Misses (F1-score):** A single score balancing Precision and Recall. The highest F1-score achieved by any model was **0.1111**. This is relatively low and highlights the difficulty in simultaneously minimizing false alarms and catching most failures with the current approach.\n",
        "\n",
        "**Visualizing the Trade-off:** (This graph shows that improving the rate of catching failures (Recall) currently increases the rate of false alarms (reduces Precision), and vice versa. The ideal scenario is high on both, but this isn't achieved yet.)\n",
        "\n",
        "![Precision-Recall Curve Plot Here - from cell d81389f6](path/to/precision_recall_curve.png)\n",
        "\n",
        "**Key Business Findings:**\n",
        "\n",
        "*   We have a functional system that can identify potential high-risk products better than chance.\n",
        "*   The current system's ability to reliably flag *most* failures while keeping false alarms at a manageable level is limited.\n",
        "*   Significant improvement in catching failures without excessive false alarms requires further work.\n",
        "\n",
        "## Next Steps: Driving Better Predictions\n",
        "\n",
        "To build a more effective prediction system for deployment, we recommend focusing on:\n",
        "\n",
        "*   **Expand Data Usage:** Explore ways to leverage more of the full dataset for training, potentially uncovering richer patterns.\n",
        "*   **Refine Data Insights:** Investigate more advanced data preparation techniques and incorporate manufacturing expertise to create stronger signals related to failure.\n",
        "*   **Optimize for Business Impact:** Explore modeling techniques that can be specifically tuned to minimize the most costly type of error (e.g., missing a critical failure).\n",
        "\n",
        "This project provides a solid foundation. Addressing these areas will be key to developing a prediction system that can significantly reduce product failures and associated costs in our manufacturing process."
      ],
      "id": "f8a1109d"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b20b2b76"
      },
      "source": [
        "# Project Summary for Business Audience: Predicting Product Failures\n",
        "\n",
        "This report summarizes our project to predict product failures in manufacturing, focusing on the business impact and future potential.\n",
        "\n",
        "## 1. Problem Statement: Minimizing Costly Failures\n",
        "\n",
        "Our core problem is to proactively identify products likely to fail during or after production. Undetected failures are costly, leading to rework, warranty expenses, and damage to our brand. A successful prediction system would allow us to intervene early, improving quality and reducing waste. A key challenge is the rarity of failures â€“ predicting these few events among many good products is difficult.\n",
        "\n",
        "## 2. Model Outcomes: Identifying Risk\n",
        "\n",
        "Our system uses data analysis to predict whether a product is high-risk (likely to fail) or low-risk (likely to be good). It provides a risk score for each product. This is a supervised learning approach, meaning the system learned from past examples of products that either failed or did not fail.\n",
        "\n",
        "## 3. Data Used: Learning from Production Signals\n",
        "\n",
        "We used data from our production line, specifically the Bosch Production Line Performance dataset. This data contains valuable signals and measurements captured during manufacturing.\n",
        "\n",
        "*Note: Due to the immense size and complexity of the original dataset, our analysis was conducted on a representative sample. While a full analysis would ideally use all available data, this approach allowed us to build and evaluate initial models within practical limits.*\n",
        "\n",
        "**Visualizing Data Potential:** (To be filled with business-friendly visuals from initial data exploration, if available. E.g., graphs showing the distribution of good vs. bad products, patterns in key measurements related to failures.)\n",
        "\n",
        "![Initial Data Acquisition Visuals Placeholder](path/to/initial_data_acquisition_visuals.png)\n",
        "\n",
        "![EDA Visuals Placeholder](path/to/eda_visuals.png)\n",
        "\n",
        "\n",
        "## 4. Data Preparation: Getting Data Ready for Prediction\n",
        "\n",
        "We prepared the raw production data for analysis:\n",
        "\n",
        "*   **Cleaning Missing Information:** We addressed gaps in the data using automated methods to ensure the models received complete information.\n",
        "*   **Structuring Data:** We organized the data and created new measurements (e.g., time-based signals) to help the models find patterns.\n",
        "*   **Splitting for Testing:** We set aside a portion of the data as a \"blind test\" to ensure our models work well on products they haven't seen before, simulating real-world performance.\n",
        "\n",
        "*Approach Note: While we utilized advanced data preparation techniques, not all possible advanced methods were explored due to the scope of effort, resources, and time allotted for this project.*\n",
        "\n",
        "## 5. Predictive Models Explored\n",
        "\n",
        "We evaluated several predictive modeling techniques suitable for identifying risk based on complex data. We focused on methods capable of handling the challenge of rare failures, including standard models, tuned versions, and specialized techniques for imbalanced data.\n",
        "\n",
        "## 6. Model Performance: Strengths and Areas for Improvement\n",
        "\n",
        "We measured our models' performance using metrics relevant to identifying failures:\n",
        "\n",
        "*   **Overall Risk Ranking (AUC):** How well the model ranks high-risk products above low-risk ones. Our best model achieved an AUC of **0.6522**, showing a better-than-random ability to differentiate risk, but indicating room for improvement in its overall discriminative power.\n",
        "*   **Minimizing False Alarms (Precision):** When the model flags a product as high-risk, how often is it genuinely faulty? Our analysis showed the highest Precision achieved was **0.3333** (meaning 1 in 3 flagged products were actual failures) with one model, but this often came at the cost of missing many actual failures. With our primary selected model (optimized for overall risk ranking), Precision was **0.1111**.\n",
        "*   **Catching Actual Failures (Recall):** Out of all the products that actually failed, how many did our model successfully flag as high-risk? With our primary selected model, Recall was **0.0667**. To catch more failures, we could increase Recall (e.g., to **0.80**), but this drastically increases false alarms, driving Precision down to **0.0102**.\n",
        "*   **Balancing Alarms and Misses (F1-score):** A single score balancing Precision and Recall. The highest F1-score achieved by any model was **0.1111**. This is relatively low and highlights the difficulty in simultaneously minimizing false alarms and catching most failures with the current approach.\n",
        "\n",
        "**Visualizing the Trade-off:** (This graph shows that improving the rate of catching failures (Recall) currently increases the rate of false alarms (reduces Precision), and vice versa. The ideal scenario is high on both, but this isn't achieved yet.)\n",
        "\n",
        "![Precision-Recall Curve Plot Here - from cell d81389f6](path/to/precision_recall_curve.png)\n",
        "\n",
        "**Key Business Findings:**\n",
        "\n",
        "*   We have a functional system that can identify potential high-risk products better than chance.\n",
        "*   The current system's ability to reliably flag *most* failures while keeping false alarms at a manageable level is limited.\n",
        "*   Significant improvement in catching failures without excessive false alarms requires further work.\n",
        "\n",
        "## Next Steps: Driving Better Predictions\n",
        "\n",
        "To build a more effective prediction system for deployment, we recommend focusing on:\n",
        "\n",
        "*   **Expand Data Usage:** Explore ways to leverage more of the full dataset for training, potentially uncovering richer patterns.\n",
        "*   **Refine Data Insights:** Investigate more advanced data preparation techniques and incorporate manufacturing expertise to create stronger signals related to failure.\n",
        "*   **Optimize for Business Impact:** Explore modeling techniques that can be specifically tuned to minimize the most costly type of error (e.g., missing a critical failure).\n",
        "\n",
        "This project provides a solid foundation. Addressing these areas will be key to developing a prediction system that can significantly reduce product failures and associated costs in our manufacturing process."
      ],
      "id": "b20b2b76"
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================\n",
        "# ðŸ“Š Generate Major Evaluation Graphs For README\n",
        "# ============================================\n",
        "\n",
        "# Ensure figures folder exists\n",
        "os.makedirs(\"figures\", exist_ok=True)\n",
        "\n",
        "# --------------------------------------------\n",
        "# 1. Confusion Matrix\n",
        "# --------------------------------------------\n",
        "y_pred = best_tuned_model.predict(X_test_imputed)\n",
        "cm = confusion_matrix(y_test, y_pred)\n",
        "disp = ConfusionMatrixDisplay(confusion_matrix=cm)\n",
        "disp.plot(cmap=\"Blues\", values_format=\"d\")\n",
        "plt.title(\"Confusion Matrix - Tuned Gradient Boosting\")\n",
        "plt.savefig(\"figures/confusion_matrix.png\", dpi=300, bbox_inches=\"tight\")\n",
        "plt.show()\n",
        "\n",
        "# --------------------------------------------\n",
        "# 2. Precisionâ€“Recall Curve\n",
        "# --------------------------------------------\n",
        "y_proba = best_tuned_model.predict_proba(X_test_imputed)[:, 1]\n",
        "prec, rec, thresh = precision_recall_curve(y_test, y_proba)\n",
        "\n",
        "plt.figure(figsize=(7,6))\n",
        "plt.plot(rec, prec, color=\"blue\", lw=2)\n",
        "plt.xlabel(\"Recall\")\n",
        "plt.ylabel(\"Precision\")\n",
        "plt.title(\"Precisionâ€“Recall Curve - Tuned Gradient Boosting\")\n",
        "plt.grid(True)\n",
        "plt.savefig(\"figures/precision_recall_curve.png\", dpi=300, bbox_inches=\"tight\")\n",
        "plt.show()\n",
        "\n",
        "# --------------------------------------------\n",
        "# 3. Feature Importance (Top 20)\n",
        "# --------------------------------------------\n",
        "importances = best_tuned_model.feature_importances_\n",
        "indices = np.argsort(importances)[-20:]  # top 20 features\n",
        "\n",
        "plt.figure(figsize=(8,6))\n",
        "plt.barh(range(len(indices)), importances[indices], align=\"center\", color=\"green\")\n",
        "plt.yticks(range(len(indices)), [f\"Feature {i}\" for i in indices])\n",
        "plt.xlabel(\"Importance\")\n",
        "plt.title(\"Top 20 Feature Importances - Tuned Gradient Boosting\")\n",
        "plt.tight_layout()\n",
        "plt.savefig(\"figures/feature_importance.png\", dpi=300, bbox_inches=\"tight\")\n",
        "plt.show()\n",
        "\n",
        "# --------------------------------------------\n",
        "# 4. Learning Curve\n",
        "# --------------------------------------------\n",
        "train_sizes, train_scores, test_scores = learning_curve(\n",
        "    best_tuned_model, X_train_imputed, y_train, cv=3, scoring=\"roc_auc\", n_jobs=-1\n",
        ")\n",
        "\n",
        "train_mean = np.mean(train_scores, axis=1)\n",
        "test_mean = np.mean(test_scores, axis=1)\n",
        "\n",
        "plt.figure(figsize=(8,6))\n",
        "plt.plot(train_sizes, train_mean, \"o-\", color=\"red\", label=\"Training AUC\")\n",
        "plt.plot(train_sizes, test_mean, \"o-\", color=\"green\", label=\"Validation AUC\")\n",
        "plt.xlabel(\"Training Examples\")\n",
        "plt.ylabel(\"AUC\")\n",
        "plt.title(\"Learning Curve - Tuned Gradient Boosting\")\n",
        "plt.legend(loc=\"best\")\n",
        "plt.grid(True)\n",
        "plt.savefig(\"figures/learning_curve.png\", dpi=300, bbox_inches=\"tight\")\n",
        "plt.show()\n",
        "\n",
        "# --------------------------------------------\n",
        "# 5. Model Metrics Comparison (using actual notebook results)\n",
        "# --------------------------------------------\n",
        "metrics = {\n",
        "    \"Logistic Regression\": [0.9943, 0.0000, 0.0000, 0.0000, 0.5170],\n",
        "    \"Random Forest\": [0.9939, 0.0000, 0.0000, 0.0000, 0.6038],\n",
        "    \"Gradient Boosting\": [0.9908, 0.0909, 0.0667, 0.0769, 0.6357],\n",
        "    \"SVC\": [0.9943, 0.0000, 0.0000, 0.0000, 0.5009],\n",
        "    \"KNN\": [0.9943, 0.0000, 0.0000, 0.0000, 0.5186],\n",
        "    \"Tuned GB\": [0.9939, 0.3333, 0.0667, 0.1111, 0.6368]\n",
        "}\n",
        "\n",
        "df = pd.DataFrame(metrics, index=[\"Accuracy\",\"Precision\",\"Recall\",\"F1\",\"AUC\"]).T\n",
        "\n",
        "df.plot(kind=\"bar\", figsize=(10,6))\n",
        "plt.title(\"Model Metrics Comparison\")\n",
        "plt.ylabel(\"Score\")\n",
        "plt.xticks(rotation=45, ha=\"right\")\n",
        "plt.legend(loc=\"upper right\")\n",
        "plt.tight_layout()\n",
        "plt.savefig(\"figures/model_metrics.png\", dpi=300, bbox_inches=\"tight\")\n",
        "plt.show()\n",
        "\n",
        "# --------------------------------------------\n",
        "# 6. Threshold Sweep (Precision, Recall, F1 vs Threshold)\n",
        "# --------------------------------------------\n",
        "thresholds = np.linspace(0.0, 1.0, 50)\n",
        "precisions, recalls, f1s = [], [], []\n",
        "\n",
        "for t in thresholds:\n",
        "    y_pred_thresh = (y_proba >= t).astype(int)\n",
        "    precisions.append(precision_score(y_test, y_pred_thresh, zero_division=0))\n",
        "    recalls.append(recall_score(y_test, y_pred_thresh, zero_division=0))\n",
        "    f1s.append(f1_score(y_test, y_pred_thresh, zero_division=0))\n",
        "\n",
        "plt.figure(figsize=(8,6))\n",
        "plt.plot(thresholds, precisions, label=\"Precision\", color=\"blue\", marker=\"o\", markersize=3)\n",
        "plt.plot(thresholds, recalls, label=\"Recall\", color=\"green\", marker=\"o\", markersize=3)\n",
        "plt.plot(thresholds, f1s, label=\"F1-score\", color=\"red\", marker=\"o\", markersize=3)\n",
        "plt.xlabel(\"Decision Threshold\")\n",
        "plt.ylabel(\"Score\")\n",
        "plt.title(\"Precisionâ€“Recallâ€“F1 vs Threshold - Tuned Gradient Boosting\")\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.savefig(\"figures/threshold_sweep.png\", dpi=300, bbox_inches=\"tight\")\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "farzinFoxcrD"
      },
      "id": "farzinFoxcrD",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Hv6iOOwh0w95"
      },
      "id": "Hv6iOOwh0w95",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}